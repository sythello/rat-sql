{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import _jsonnet\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/third_party/wikisql'))\n",
    "sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Projects-M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.datasets.spider import SpiderItem\n",
    "from ratsql.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeakQL.SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_beam_size': 1,\n",
       " 'eval_name': 'glove_run_true_1',\n",
       " 'eval_output': '__LOGDIR__/ie_dirs',\n",
       " 'eval_section': 'val',\n",
       " 'eval_steps': [30100,\n",
       "  31100,\n",
       "  32100,\n",
       "  33100,\n",
       "  34100,\n",
       "  35100,\n",
       "  36100,\n",
       "  37100,\n",
       "  38100,\n",
       "  39100,\n",
       "  40000],\n",
       " 'eval_use_heuristic': True,\n",
       " 'logdir': 'logdir/glove_run',\n",
       " 'model_config': 'configs/spider/nl2code-glove.jsonnet',\n",
       " 'model_config_args': {'att': 0,\n",
       "  'clause_order': None,\n",
       "  'cv_link': True,\n",
       "  'enumerate_order': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_config_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-run.jsonnet'\n",
    "exp_config = json.loads(_jsonnet.evaluate_file(exp_config_path))\n",
    "exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'att': 0, 'clause_order': None, 'cv_link': True, 'enumerate_order': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql'\n",
    "model_config_path = os.path.join(root_dir, exp_config[\"model_config\"])\n",
    "model_config_args = exp_config.get(\"model_config_args\")\n",
    "model_config_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'train': {'db_path': 'data/spider/database',\n",
       "   'name': 'spider',\n",
       "   'paths': ['data/spider/train_spider.json', 'data/spider/train_others.json'],\n",
       "   'tables_paths': ['data/spider/tables.json']},\n",
       "  'val': {'db_path': 'data/spider/database',\n",
       "   'name': 'spider',\n",
       "   'paths': ['data/spider/dev.json'],\n",
       "   'tables_paths': ['data/spider/tables.json']}},\n",
       " 'lr_scheduler': {'decay_steps': 38000,\n",
       "  'end_lr': 0,\n",
       "  'name': 'warmup_polynomial',\n",
       "  'num_warmup_steps': 2000,\n",
       "  'power': 0.5,\n",
       "  'start_lr': 0.000743552663260837},\n",
       " 'model': {'decoder': {'desc_attn': 'mha',\n",
       "   'dropout': 0.20687225956012834,\n",
       "   'enumerate_order': False,\n",
       "   'loss_type': 'softmax',\n",
       "   'name': 'NL2Code',\n",
       "   'recurrent_size': 512,\n",
       "   'use_align_loss': True,\n",
       "   'use_align_mat': True},\n",
       "  'decoder_preproc': {'grammar': {'clause_order': None,\n",
       "    'end_with_from': True,\n",
       "    'factorize_sketch': 2,\n",
       "    'include_literals': False,\n",
       "    'infer_from_conditions': True,\n",
       "    'name': 'spider',\n",
       "    'output_from': True,\n",
       "    'use_table_pointer': True},\n",
       "   'max_count': 5000,\n",
       "   'min_freq': 4,\n",
       "   'save_path': 'data/spider/nl2code-glove,cv_link=true',\n",
       "   'use_seq_elem_rules': True},\n",
       "  'encoder': {'batch_encs_update': False,\n",
       "   'column_encoder': ['emb', 'bilstm-summarize'],\n",
       "   'dropout': 0.2,\n",
       "   'name': 'spiderv2',\n",
       "   'question_encoder': ['emb', 'bilstm'],\n",
       "   'table_encoder': ['emb', 'bilstm-summarize'],\n",
       "   'top_k_learnable': 50,\n",
       "   'update_config': {'cv_link': True,\n",
       "    'name': 'relational_transformer',\n",
       "    'num_heads': 8,\n",
       "    'num_layers': 8,\n",
       "    'sc_link': True},\n",
       "   'word_emb_size': 300},\n",
       "  'encoder_preproc': {'compute_cv_link': True,\n",
       "   'compute_sc_link': True,\n",
       "   'count_tokens_in_word_emb_for_vocab': True,\n",
       "   'db_path': 'data/spider/database',\n",
       "   'fix_issue_16_primary_keys': True,\n",
       "   'include_table_name_in_column': False,\n",
       "   'max_count': 5000,\n",
       "   'min_freq': 4,\n",
       "   'save_path': 'data/spider/nl2code-glove,cv_link=true',\n",
       "   'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}},\n",
       "  'name': 'EncDec'},\n",
       " 'model_name': 'bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
       " 'optimizer': {'lr': 0, 'name': 'adam'},\n",
       " 'train': {'batch_size': 20,\n",
       "  'clip_grad': None,\n",
       "  'data_seed': 0,\n",
       "  'eval_batch_size': 50,\n",
       "  'eval_every_n': 100,\n",
       "  'init_seed': 0,\n",
       "  'keep_every_n': 1000,\n",
       "  'max_steps': 40000,\n",
       "  'model_seed': 0,\n",
       "  'num_batch_accumulated': 1,\n",
       "  'num_eval_items': 50,\n",
       "  'report_every_n': 10,\n",
       "  'save_every_n': 100}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_config = json.loads(_jsonnet.evaluate_file(model_config_path, tla_codes={'args': json.dumps(model_config_args)}))\n",
    "infer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0/model_checkpoint-00040000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|██████████| 166/166 [00:01<00:00, 133.00it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_step = 40000\n",
    "model_dir = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0'\n",
    "\n",
    "inferer = Inferer(infer_config)\n",
    "inferer.device = torch.device(\"cpu\")\n",
    "model = inferer.load_model(model_dir, checkpoint_step)\n",
    "dataset = registry.construct('dataset', inferer.config['data']['val'])\n",
    "\n",
    "for _, schema in dataset.schemas.items():\n",
    "    model.preproc.enc_preproc._preprocess_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question(q, db_id):\n",
    "    spider_schema = dataset.schemas[db_id]\n",
    "    data_item = SpiderItem(\n",
    "        text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly\n",
    "        code=None,\n",
    "        schema=spider_schema,\n",
    "        orig_schema=spider_schema.orig,\n",
    "        orig={\"question\": q}\n",
    "    )\n",
    "    model.preproc.clear_items()\n",
    "    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)\n",
    "    preproc_data = enc_input, None\n",
    "    with torch.no_grad():\n",
    "        return inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many singers do we have?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where'},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 1}]}},\n",
       "  'inferred_code': 'SELECT Count(*) FROM singer',\n",
       "  'score': -0.00011539317730324683}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_path, 'r') as f:\n",
    "    dev_dataset = json.load(f)\n",
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dev_dataset[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(d['question'], d['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sql_list = []\n",
    "\n",
    "for d in tqdm(dev_dataset):\n",
    "    pred = question(d['question'], d['db_id'])[0]\n",
    "    pred_sql_list.append(pred['inferred_code'])\n",
    "\n",
    "len(pred_sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_path = './output/dev_output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dev_pred_path, 'w') as f:\n",
    "#     for pred in pred_sql_list:\n",
    "#         f.write(pred + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict SQL given transcribed dataset JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'train'\n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon.json'.format(DATASET)\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon_RatsqlPredicted.json'.format(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for human test \n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon.json'\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon_RatsqlPredicted.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(input_dataset_path, 'r') as f:\n",
    "    asr_dataset = json.load(f)\n",
    "len(asr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'original_id', 'span_ranges'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30973cd855584866b2c8bb51a56adc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=712.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'db_id': 'pets_1',\n",
       " 'query': 'SELECT count(*) FROM pets WHERE weight  >  10',\n",
       " 'query_toks': ['SELECT',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'pets',\n",
       "  'WHERE',\n",
       "  'weight',\n",
       "  '>',\n",
       "  '10'],\n",
       " 'query_toks_no_value': ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'from',\n",
       "  'pets',\n",
       "  'where',\n",
       "  'weight',\n",
       "  '>',\n",
       "  'value'],\n",
       " 'question': 'find the number of paths whose weight is heavier than 10.',\n",
       " 'question_toks': ['find',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'paths',\n",
       "  'whose',\n",
       "  'weight',\n",
       "  'is',\n",
       "  'heavier',\n",
       "  'than',\n",
       "  '10',\n",
       "  '.'],\n",
       " 'sql': {'except': None,\n",
       "  'from': {'conds': [], 'table_units': [['table_unit', 2]]},\n",
       "  'groupBy': [],\n",
       "  'having': [],\n",
       "  'intersect': None,\n",
       "  'limit': None,\n",
       "  'orderBy': [],\n",
       "  'select': [False, [[3, [0, [0, 0, False], None]]]],\n",
       "  'union': None,\n",
       "  'where': [[False, 3, [0, [0, 14, False], None], 10.0, None]]},\n",
       " 'original_id': 0,\n",
       " 'span_ranges': [['1.34', '1.74'],\n",
       "  ['1.74', '1.85'],\n",
       "  ['1.85', '2.27'],\n",
       "  ['2.27', '2.47'],\n",
       "  ['2.47', '2.88'],\n",
       "  ['2.88', '3.22'],\n",
       "  ['3.22', '3.49'],\n",
       "  ['3.49', '3.68'],\n",
       "  ['3.68', '4.14'],\n",
       "  ['4.14', '4.33'],\n",
       "  ['4.33', '4.67'],\n",
       "  [0, 0]],\n",
       " 'ratsql_pred_sql': \"SELECT Count(*) FROM Pets WHERE Pets.weight > 'terminal'\"}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, d in tqdm(enumerate(asr_dataset), total=len(asr_dataset)):\n",
    "    if 'ratsql_pred_sql' in d:\n",
    "        continue\n",
    "        \n",
    "    pred = Question(d['question'], d['db_id'])\n",
    "    if len(pred) == 0:\n",
    "        print('{}: question({}, {}) failed'.format(i, d['question'], d['db_id']))\n",
    "        d['ratsql_pred_sql'] = ''\n",
    "    else:\n",
    "        d['ratsql_pred_sql'] = pred[0]['inferred_code']\n",
    "\n",
    "asr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = question(d['question'], d['db_id'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_dataset_path, 'w') as f:\n",
    "#     json.dump(asr_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict & Evaluate SQL given rewriter output / original / ASR cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "kmaps = evaluation.build_foreign_key_map_from_json(tables_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateSQL(pred_str,\n",
    "                gold_str,\n",
    "                db,\n",
    "                db_dir='/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database/'):\n",
    "    \n",
    "    db_path = os.path.join(db_dir, db, db + \".sqlite\")\n",
    "    schema = process_sql.Schema(process_sql.get_schema(db_path))\n",
    "    try:\n",
    "        g_sql = process_sql.get_sql(schema, gold_str)  # Train #3153/18259, in 'assets_maintenance', 'ref_company_types' not found \n",
    "        p_sql = process_sql.get_sql(schema, pred_str)\n",
    "    except:\n",
    "        print('{}\\n{}\\n{}\\nprocess_sql.get_sql() failed'.format(pred_str, gold_str, db))\n",
    "        return 0, 0\n",
    "    \n",
    "    # Rebuilding... copied from official evaluate \n",
    "    kmap = kmaps[db]\n",
    "    g_valid_col_units = evaluation.build_valid_col_units(g_sql['from']['table_units'], schema)\n",
    "    g_sql = evaluation.rebuild_sql_val(g_sql)\n",
    "    g_sql = evaluation.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "    p_valid_col_units = evaluation.build_valid_col_units(p_sql['from']['table_units'], schema)\n",
    "    p_sql = evaluation.rebuild_sql_val(p_sql)\n",
    "    p_sql = evaluation.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "    \n",
    "    evaluator = evaluation.Evaluator()\n",
    "    exact_match = evaluator.eval_exact_match(p_sql, g_sql)   # will modify p_sql, g_sql\n",
    "    partials = evaluator.partial_scores\n",
    "    partial_summary_score = sum([partials[tp]['f1'] * max(partials[tp]['label_total'], partials[tp]['pred_total']) for tp in partials]) / sum([max(partials[tp]['label_total'], partials[tp]['pred_total']) for tp in partials])\n",
    "\n",
    "    return int(exact_match), partial_summary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateSQL_full(glist,\n",
    "                     plist,\n",
    "                     db_id_list,\n",
    "                     kmaps,\n",
    "                     db_dir='/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database/'):\n",
    "    # Only using 'match', not using 'exec'\n",
    "    etype = 'match'\n",
    "    \n",
    "    # with open(gold) as f:\n",
    "    #     glist = [l.strip().split('\\t') for l in f.readlines() if len(l.strip()) > 0]\n",
    "\n",
    "    # with open(predict) as f:\n",
    "    #     plist = [l.strip().split('\\t') for l in f.readlines() if len(l.strip()) > 0]\n",
    "    \n",
    "    # plist = [(\"select max(Share),min(Share) from performance where Type != 'terminal'\", \"orchestra\")]\n",
    "    # glist = [(\"SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != 'Live final'\", \"orchestra\")]\n",
    "    evaluator = evaluation.Evaluator()\n",
    "\n",
    "    levels = ['easy', 'medium', 'hard', 'extra', 'all']\n",
    "    partial_types = ['select', 'select(no AGG)', 'where', 'where(no OP)', 'group(no Having)',\n",
    "                     'group', 'order', 'and/or', 'IUEN', 'keywords']\n",
    "    entries = []\n",
    "    scores = {}\n",
    "\n",
    "    for level in levels:\n",
    "        scores[level] = {'count': 0, 'partial': {}, 'exact': 0.}\n",
    "        scores[level]['exec'] = 0\n",
    "        for type_ in partial_types:\n",
    "            scores[level]['partial'][type_] = {'acc': 0., 'rec': 0., 'f1': 0.,'acc_count':0,'rec_count':0}\n",
    "\n",
    "    eval_err_num = 0\n",
    "    for p, g, db in tqdm(zip(plist, glist, db_id_list), total=len(plist)):\n",
    "        p_str = p\n",
    "        g_str = g\n",
    "        db_name = db\n",
    "        db = os.path.join(db_dir, db, db + \".sqlite\")\n",
    "        schema = process_sql.Schema(process_sql.get_schema(db))\n",
    "        g_sql = process_sql.get_sql(schema, g_str)\n",
    "        hardness = evaluator.eval_hardness(g_sql)\n",
    "        scores[hardness]['count'] += 1\n",
    "        scores['all']['count'] += 1\n",
    "\n",
    "        try:\n",
    "            p_sql = process_sql.get_sql(schema, p_str)\n",
    "        except:\n",
    "            # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "            p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\n",
    "                \"conds\": [],\n",
    "                \"table_units\": []\n",
    "            },\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [\n",
    "                False,\n",
    "                []\n",
    "            ],\n",
    "            \"union\": None,\n",
    "            \"where\": []\n",
    "            }\n",
    "            eval_err_num += 1\n",
    "            print(\"eval_err_num:{}\".format(eval_err_num))\n",
    "\n",
    "        # rebuild sql for value evaluation\n",
    "        kmap = kmaps[db_name]\n",
    "        g_valid_col_units = evaluation.build_valid_col_units(g_sql['from']['table_units'], schema)\n",
    "        g_sql = evaluation.rebuild_sql_val(g_sql)\n",
    "        g_sql = evaluation.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "        p_valid_col_units = evaluation.build_valid_col_units(p_sql['from']['table_units'], schema)\n",
    "        p_sql = evaluation.rebuild_sql_val(p_sql)\n",
    "        p_sql = evaluation.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "\n",
    "#         if etype in [\"all\", \"exec\"]:\n",
    "#             exec_score = evaluation.eval_exec_match(db, p_str, g_str, p_sql, g_sql)\n",
    "#             if exec_score:\n",
    "#                 scores[hardness]['exec'] += 1\n",
    "\n",
    "        if etype in [\"all\", \"match\"]:\n",
    "            exact_score = evaluator.eval_exact_match(p_sql, g_sql)\n",
    "            partial_scores = evaluator.partial_scores\n",
    "#             if exact_score == 0:\n",
    "#                 print(\"{} pred: {}\".format(hardness,p_str))\n",
    "#                 print(\"{} gold: {}\".format(hardness,g_str))\n",
    "#                 print(\"\")\n",
    "            scores[hardness]['exact'] += exact_score\n",
    "            scores['all']['exact'] += exact_score\n",
    "            for type_ in partial_types:\n",
    "                if partial_scores[type_]['pred_total'] > 0:\n",
    "                    scores[hardness]['partial'][type_]['acc'] += partial_scores[type_]['acc']\n",
    "                    scores[hardness]['partial'][type_]['acc_count'] += 1\n",
    "                if partial_scores[type_]['label_total'] > 0:\n",
    "                    scores[hardness]['partial'][type_]['rec'] += partial_scores[type_]['rec']\n",
    "                    scores[hardness]['partial'][type_]['rec_count'] += 1\n",
    "                scores[hardness]['partial'][type_]['f1'] += partial_scores[type_]['f1']\n",
    "                if partial_scores[type_]['pred_total'] > 0:\n",
    "                    scores['all']['partial'][type_]['acc'] += partial_scores[type_]['acc']\n",
    "                    scores['all']['partial'][type_]['acc_count'] += 1\n",
    "                if partial_scores[type_]['label_total'] > 0:\n",
    "                    scores['all']['partial'][type_]['rec'] += partial_scores[type_]['rec']\n",
    "                    scores['all']['partial'][type_]['rec_count'] += 1\n",
    "                scores['all']['partial'][type_]['f1'] += partial_scores[type_]['f1']\n",
    "\n",
    "            entries.append({\n",
    "                'predictSQL': p_str,\n",
    "                'goldSQL': g_str,\n",
    "                'hardness': hardness,\n",
    "                'exact': exact_score,\n",
    "                'partial': partial_scores\n",
    "            })\n",
    "\n",
    "    for level in levels:\n",
    "        if scores[level]['count'] == 0:\n",
    "            continue\n",
    "        if etype in [\"all\", \"exec\"]:\n",
    "            scores[level]['exec'] /= scores[level]['count']\n",
    "\n",
    "        if etype in [\"all\", \"match\"]:\n",
    "            scores[level]['exact'] /= scores[level]['count']\n",
    "            for type_ in partial_types:\n",
    "                if scores[level]['partial'][type_]['acc_count'] == 0:\n",
    "                    scores[level]['partial'][type_]['acc'] = 0\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['acc'] = scores[level]['partial'][type_]['acc'] / \\\n",
    "                                                             scores[level]['partial'][type_]['acc_count'] * 1.0\n",
    "                if scores[level]['partial'][type_]['rec_count'] == 0:\n",
    "                    scores[level]['partial'][type_]['rec'] = 0\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['rec'] = scores[level]['partial'][type_]['rec'] / \\\n",
    "                                                             scores[level]['partial'][type_]['rec_count'] * 1.0\n",
    "                if scores[level]['partial'][type_]['acc'] == 0 and scores[level]['partial'][type_]['rec'] == 0:\n",
    "                    scores[level]['partial'][type_]['f1'] = 1\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['f1'] = \\\n",
    "                        2.0 * scores[level]['partial'][type_]['acc'] * scores[level]['partial'][type_]['rec'] / (\n",
    "                        scores[level]['partial'][type_]['rec'] + scores[level]['partial'][type_]['acc'])\n",
    "\n",
    "    evaluation.print_scores(scores, etype)\n",
    "    \n",
    "    # return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457fdfbaf6104bb695c1581c70823ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Just using the 1st ASR candidate, no correction \n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['question'], _db_id)[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7274 (std = 0.3069)\n",
      "avg_exact = 0.4552\n",
      "BLEU = 0.8010\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3ea238dcfa437f9eddff3572c5d9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['gold_question'], _db_id)[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.8316 (std = 0.2704)\n",
      "avg_exact = 0.6234\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35c6dc63a1747568886c7ddc76e4a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.787                0.658                0.462                0.425                0.623               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.926                0.812                0.934                0.838                0.864               \n",
      "select(no AGG)       0.963                0.820                0.934                0.838                0.877               \n",
      "where                0.776                0.744                0.627                0.426                0.677               \n",
      "where(no OP)         0.821                0.752                0.745                0.553                0.734               \n",
      "group(no Having)     0.812                0.797                0.810                0.806                0.803               \n",
      "group                0.750                0.757                0.762                0.778                0.762               \n",
      "order                1.000                0.650                0.781                0.947                0.805               \n",
      "and/or               1.000                0.958                0.955                0.863                0.954               \n",
      "IUEN                 0.000                0.000                0.350                0.500                0.395               \n",
      "keywords             0.945                0.902                0.890                0.850                0.899               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.926                0.808                0.934                0.838                0.863               \n",
      "select(no AGG)       0.963                0.817                0.934                0.838                0.876               \n",
      "where                0.800                0.777                0.640                0.417                0.695               \n",
      "where(no OP)         0.846                0.786                0.760                0.542                0.753               \n",
      "group(no Having)     0.812                0.776                0.773                0.763                0.776               \n",
      "group                0.750                0.737                0.727                0.737                0.737               \n",
      "order                1.000                0.722                0.806                0.947                0.841               \n",
      "and/or               0.963                0.991                0.966                0.972                0.977               \n",
      "IUEN                 0.000                0.000                0.292                0.500                0.375               \n",
      "keywords             0.966                0.907                0.890                0.850                0.905               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.926                0.810                0.934                0.838                0.864               \n",
      "select(no AGG)       0.963                0.818                0.934                0.838                0.876               \n",
      "where                0.788                0.760                0.634                0.421                0.686               \n",
      "where(no OP)         0.833                0.769                0.752                0.547                0.743               \n",
      "group(no Having)     0.812                0.787                0.791                0.784                0.789               \n",
      "group                0.750                0.747                0.744                0.757                0.749               \n",
      "order                1.000                0.684                0.794                0.947                0.823               \n",
      "and/or               0.981                0.974                0.960                0.914                0.965               \n",
      "IUEN                 1.000                1.000                0.318                0.500                0.385               \n",
      "keywords             0.956                0.904                0.890                0.850                0.902               \n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 kmaps=kmaps\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postprocess_rewrite_seq(tags, rewrite_seq, question_toks):\n",
    "    _question_toks_placeholders = []\n",
    "\n",
    "    for i, tok in enumerate(question_toks):\n",
    "        if tags[i].endswith('KEEP'):\n",
    "            _question_toks_placeholders.append(tok)\n",
    "        elif (tags[i] == 'U-EDIT') or (tags[i] == 'B-EDIT'):\n",
    "            _question_toks_placeholders.append('[EDIT]')\n",
    "        elif (tags[i] == 'I-EDIT') or (tags[i] == 'L-EDIT') or tags[i].endswith('DEL'):\n",
    "            pass\n",
    "        else:\n",
    "            print('Unknown tag: {}'.format(tags[i]))\n",
    "\n",
    "    _edits = []\n",
    "    _curr_edit = []\n",
    "    for tok in rewrite_seq:\n",
    "        if tok == '[ANS]':\n",
    "            _edits.append(_curr_edit)\n",
    "            _curr_edit = []\n",
    "        elif tok == '@end@':  # Allennlp END_SYMBOL \n",
    "            break\n",
    "        else:\n",
    "            _curr_edit.append(tok)\n",
    "    \n",
    "    _question_toks_rewritten = []\n",
    "    _edit_idx = 0\n",
    "    for tok in _question_toks_placeholders:\n",
    "        if tok == '[EDIT]':\n",
    "            if _edit_idx >= len(_edits):\n",
    "                print('--- Not enough edits ---')\n",
    "                print('Tags:', tags)\n",
    "                print('Edits:', _edits)\n",
    "            else:\n",
    "                _question_toks_rewritten.extend(_edits[_edit_idx])\n",
    "            _edit_idx += 1\n",
    "        else:\n",
    "            _question_toks_rewritten.append(tok)\n",
    "\n",
    "    return _question_toks_rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "\n",
    "rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "    rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _p in rewriter_ILM_preds[::100]:\n",
    "    print(_p['rewrite_seq_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6ab2d3e7384049b84d641ddad3ea28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['``'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-DEL', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['``'], ['USA'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['4500000'], ['?']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    p = rewriter_ILM_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "    # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "    \n",
    "    _tags = p['rewriter_tags']\n",
    "    _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    _question_toks = c['question_toks']\n",
    "\n",
    "    _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "    _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "\n",
    "    _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_rewritten_question_toks)\n",
    "    \n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7559 (std = 0.2968)\n",
      "avg_exact = 0.5027\n",
      "BLEU = 0.8562\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading from predicted file (only 1st cand is predicted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547,\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_score', 'question_toks_edit_distance', 'alignment_span_pairs', 'alignment_text_pairs', 'rewriter_tags', 'rewriter_edits', 'rewritten_question', 'pred_sql', 'score']))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION = '3.1.1_ep0'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3f051699134020bfcd8a3042d3f644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.588                0.483                0.418                0.375                0.483               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.860                0.736                0.868                0.800                0.799               \n",
      "select(no AGG)       0.890                0.749                0.868                0.800                0.811               \n",
      "where                0.623                0.604                0.531                0.348                0.547               \n",
      "where(no OP)         0.642                0.613                0.653                0.478                0.602               \n",
      "group(no Having)     0.750                0.743                0.850                0.806                0.774               \n",
      "group                0.688                0.689                0.800                0.778                0.726               \n",
      "order                0.667                0.550                0.800                0.947                0.752               \n",
      "and/or               1.000                0.928                0.933                0.863                0.937               \n",
      "IUEN                 0.000                0.000                0.318                0.467                0.341               \n",
      "keywords             0.863                0.850                0.875                0.825                0.852               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.860                0.733                0.868                0.800                0.797               \n",
      "select(no AGG)       0.890                0.746                0.868                0.800                0.810               \n",
      "where                0.508                0.571                0.520                0.333                0.505               \n",
      "where(no OP)         0.523                0.580                0.640                0.458                0.556               \n",
      "group(no Having)     0.750                0.724                0.773                0.763                0.743               \n",
      "group                0.688                0.671                0.727                0.737                0.697               \n",
      "order                0.750                0.611                0.774                0.947                0.779               \n",
      "and/or               0.963                0.982                0.976                0.972                0.975               \n",
      "IUEN                 0.000                0.000                0.292                0.438                0.350               \n",
      "keywords             0.775                0.818                0.846                0.825                0.816               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.860                0.735                0.868                0.800                0.798               \n",
      "select(no AGG)       0.890                0.747                0.868                0.800                0.811               \n",
      "where                0.559                0.587                0.525                0.340                0.526               \n",
      "where(no OP)         0.576                0.596                0.646                0.468                0.578               \n",
      "group(no Having)     0.750                0.733                0.810                0.784                0.758               \n",
      "group                0.688                0.680                0.762                0.757                0.711               \n",
      "order                0.706                0.579                0.787                0.947                0.765               \n",
      "and/or               0.981                0.954                0.954                0.914                0.955               \n",
      "IUEN                 1.000                1.000                0.304                0.452                0.346               \n",
      "keywords             0.817                0.833                0.860                0.825                0.834               \n"
     ]
    }
   ],
   "source": [
    "# Using EvaluateSQL_full \n",
    "\n",
    "tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "kmaps = evaluation.build_foreign_key_map_from_json(tables_json)\n",
    "\n",
    "plist = [d[0]['pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using all ASR candidates (no longer in use)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']\n",
    "        _tags = p['tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6206100986000569\n",
      "0.6206100986000569\n",
      "avg = 0.6842 (std = 0.2883)\n"
     ]
    }
   ],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation process with oracle tags (no longer in use for version>=2.3.0)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _tags = p['gold_tags']\n",
    "        _rewrite_seq = p['oracle_tags_rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question'] = _rewritten_question\n",
    "        c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql'] = _pred_sql\n",
    "        c['oracle_tags_score'] = p['oracle_tags_score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6416376618227428\n",
      "0.6416376618227428\n",
      "avg = 0.6904 (std = 0.2861)\n"
     ]
    }
   ],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['oracle_tags_score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['oracle_tags_score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_oracle_avg_1st = sum([d[0]['oracle_tags_score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_oracle_std_1st = np.std([d[0]['oracle_tags_score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_oracle_avg_1st, _oracle_std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['tag_prediction'] = list(zip(p['question'].split(' '), p['tags_prediction']))\n",
    "        _pred_c['rewrite_seq'] = []\n",
    "        for t in p['rewrite_seq_prediction']:\n",
    "            _pred_c['rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['rewritten_question'] = p['rewritten_question']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_tags'] = list(zip(p['question'].split(' '), p['gold_tags']))\n",
    "        _pred_c['oracle_tags_rewrite_seq'] = []\n",
    "        for t in p['oracle_tags_rewrite_seq_prediction']:\n",
    "            _pred_c['oracle_tags_rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question']\n",
    "        _pred_c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql']\n",
    "        _pred_c['oracle_tags_score'] = p['oracle_tags_score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test-prediction-{}.json'.format(VERSION), 'w') as f:\n",
    "    json.dump(test_pred_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset file with predictions \n",
    "\n",
    "with open('./output/pred-{}.json'.format(VERSION), 'r') as f:\n",
    "    test_pred_dataset = json.load(f)\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis \n",
    "orig_dev_preds_path = './output/dev_output.txt'\n",
    "\n",
    "with open(orig_dev_preds_path, 'r') as f:\n",
    "    orig_dev_preds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "len(orig_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for d in test_dataset[5::40]:\n",
    "#     print('DB:', d[0]['db_id'])\n",
    "#     print('ASR question:\\t\\t', d[0]['question'])\n",
    "#     print('Rewritten question:\\t', d[0]['rewritten_question'])\n",
    "#     print('Gold question:\\t\\t', d[0]['gold_question'])\n",
    "#     print('ASR-q Pred SQL:\\t\\t', d[0]['ratsql_pred_sql'])\n",
    "#     print('Rewritten-q Pred SQL:\\t', d[0]['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', orig_dev_preds[d[0]['original_id']])\n",
    "#     print('Gold SQL:\\t\\t', d[0]['query'])\n",
    "#     print('Score:', d[0]['score'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_samples = [c for d in test_pred_dataset for c in d]\n",
    "for i, c in list(enumerate(test_pred_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['ASR_question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "    print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['gold_sql'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = [8, 96, 272, 448, 1416, 1592, 1680, 1856, 2120, 2296, 2384, 2560, 2824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: VERSION = 3.1.1_ep0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ca1533b2664770bde438ede44b95b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.1.1_ep0:\n",
      "avg = 0.7466 (std = 0.3027)\n",
      "avg_exact = 0.4826\n",
      "BLEU = 0.8583\n",
      "\n",
      "Evaluating: VERSION = 3.1.1.1_ep0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45ee7a6f51b49eb87c82c2a610f91c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.1.1.1_ep0:\n",
      "avg = 0.7549 (std = 0.2994)\n",
      "avg_exact = 0.4918\n",
      "BLEU = 0.8445\n",
      "\n",
      "Evaluating: VERSION = 3.1.1.2_ep0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68283e6b7124423f9e198e976cadbe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.1.1.2_ep0:\n",
      "avg = 0.7408 (std = 0.3073)\n",
      "avg_exact = 0.4680\n",
      "BLEU = 0.8412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Batch evaluating \n",
    "\n",
    "VERSION_LIST = ['3.1.1_ep0', '3.1.1.1_ep0', '3.1.1.2_ep0']\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    print(f'Evaluating: VERSION = {VERSION}')\n",
    "    rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "\n",
    "    with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "        rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "        \n",
    "    # Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    \n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags']\n",
    "        # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        # _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "\n",
    "        if _rewritten_question == '':\n",
    "            print(f'_rewritten_question is empty')\n",
    "            _pred_sql = ''\n",
    "            _gold_sql = c['query']\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "            _gold_sql = c['query']\n",
    "            _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        \n",
    "        # For BLEU \n",
    "        _rewritten_question_toks = [_t.lower() for _t in p['s2s_prediction']]\n",
    "        _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "    \n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    \n",
    "    print(f'VERSION {VERSION}:')\n",
    "    print(f'avg = {_avg_1st:.4f} (std = {_std_1st:.4f})')\n",
    "    print(f'avg_exact = {_avg_exact_1st:.4f}')\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print()\n",
    "    \n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.3.0.0'\n",
    "\n",
    "rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "    rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'original_id', 'gold_rewrite_seq_s2s', 's2s_prediction', 's2s_prediction_cands', 'rewrite_seq_NLL'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewriter_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_preds[3::300]:\n",
    "    print(p['question'])\n",
    "    print(' '.join(p['s2s_prediction']))\n",
    "    print(' '.join(p['gold_rewrite_seq_s2s'][1:-1]))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2693a9086743cea09227c2ad4fcc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags']\n",
    "    # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    # _question_toks = c['question_toks']\n",
    "\n",
    "    # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "    \n",
    "    if _rewritten_question == '':\n",
    "        print(f'_rewritten_question is empty')\n",
    "        _pred_sql = ''\n",
    "        _gold_sql = c['query']\n",
    "        _score = 0\n",
    "    else:\n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "\n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.6914 (std = 0.2852)\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actual (full) evaluation process \n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        pred_idx += 1\n",
    "        if 'score' in c:\n",
    "            continue  # already inferred  \n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "        _pred_result = Question(_rewritten_question, _db_id)\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        \n",
    "        if len(_pred_result) == 0:\n",
    "            print(_db_id, _rewritten_question, '-- no predictiction')\n",
    "            _pred_sql = ''\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = _pred_result[0]['inferred_code']\n",
    "            _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_preds]) / len(rewriter_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['rewritten_question'] = p['s2s_prediction']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ ID = 8 ------------------------------\n",
      "ASR question:\t\t how many pets have a greater wait than 10?\n",
      "Rewritten question:\t How many pets have a greater branches ?\n",
      "Gold question:\t\t How many pets have a greater weight than 10?\n",
      "Rewritten-q Pred SQL:\t SELECT Count(*) FROM Pets\n",
      "Gold SQL:\t\t SELECT count(*) FROM pets WHERE weight  >  10\n",
      "Score: 0.5\n",
      "------------------------------ ID = 96 ------------------------------\n",
      "ASR question:\t\t find the name of students who had both kept in Doug pets.\n",
      "Rewritten question:\t Find the name of students who have both 50000 or more Doug pets .\n",
      "Gold question:\t\t Find the name of students who have both cat and dog pets.\n",
      "Rewritten-q Pred SQL:\t SELECT Student.Fname FROM Student JOIN Has_Pet ON Student.StuID = Has_Pet.StuID JOIN Pets ON Has_Pet.PetID = Pets.PetID WHERE Pets.PetType = 'terminal' GROUP BY Student.StuID HAVING Count(*) >= 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.Fname FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat' INTERSECT SELECT T1.Fname FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'dog'\n",
      "Score: 0.36363636363636365\n",
      "------------------------------ ID = 184 ------------------------------\n",
      "ASR question:\t\t what are the different first names and ages of the students who do have pets?\n",
      "Rewritten question:\t What are the different first names and ages of the students who do have pets ?\n",
      "Gold question:\t\t What are the different first names and ages of the students who do have pets?\n",
      "Rewritten-q Pred SQL:\t SELECT DISTINCT Student.Fname, Student.Age FROM Student JOIN Has_Pet ON Student.StuID = Has_Pet.StuID\n",
      "Gold SQL:\t\t SELECT DISTINCT T1.fname ,  T1.age FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid\n",
      "Score: 1.0\n",
      "------------------------------ ID = 272 ------------------------------\n",
      "ASR question:\t\t list the airport Koda name in the city of Anthony.\n",
      "Rewritten question:\t List the airport names name and the city of Anthony .\n",
      "Gold question:\t\t List the airport code and name in the city of Anthony.\n",
      "Rewritten-q Pred SQL:\t SELECT airports.AirportName, airports.City FROM airports WHERE airports.City = 'terminal'\n",
      "Gold SQL:\t\t SELECT AirportCode ,  AirportName FROM AIRPORTS WHERE city  =  \"Anthony\"\n",
      "Score: 0.375\n",
      "------------------------------ ID = 360 ------------------------------\n",
      "ASR question:\t\t What is the airport name for airports, Rocco?\n",
      "Rewritten question:\t What is the airport name for airports made ?\n",
      "Gold question:\t\t What is the airport name for airport 'AKO'?\n",
      "Rewritten-q Pred SQL:\t SELECT airports.AirportName FROM airports\n",
      "Gold SQL:\t\t SELECT AirportName FROM AIRPORTS WHERE AirportCode  =  \"AKO\"\n",
      "Score: 0.5\n",
      "------------------------------ ID = 448 ------------------------------\n",
      "ASR question:\t\t count the number of United Airlines flights that derive an Aberdeen.\n",
      "Rewritten question:\t Count the number of United flights that do n't the matches .\n",
      "Gold question:\t\t Count the number of United Airlines flights that arrive in Aberdeen.\n",
      "Rewritten-q Pred SQL:\t SELECT Count(*) FROM airports JOIN flights ON airports.AirportCode = flights.SourceAirport WHERE airports.Country = 'terminal' AND flights.Airline NOT IN (SELECT airports.AirportCode FROM airports WHERE airports.Country = 'terminal')\n",
      "Gold SQL:\t\t SELECT count(*) FROM FLIGHTS AS T1 JOIN AIRPORTS AS T2 ON T1.DestAirport  =  T2.AirportCode JOIN AIRLINES AS T3 ON T3.uid  =  T1.Airline WHERE T2.City  =  \"Aberdeen\" AND T3.Airline  =  \"United Airlines\"\n",
      "Score: 0.3\n",
      "------------------------------ ID = 536 ------------------------------\n",
      "ASR question:\t\t find our airlines that have fewer than 200 flights.\n",
      "Rewritten question:\t Find the airlines that have fewer than 200 flights .\n",
      "Gold question:\t\t Find all airlines that have fewer than 200 flights.\n",
      "Rewritten-q Pred SQL:\t SELECT airlines.Airline FROM airlines JOIN flights GROUP BY airlines.uid HAVING Count(*) < 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid  =  T2.Airline GROUP BY T1.Airline HAVING count(*)  <  200\n",
      "Score: 0.7142857142857143\n",
      "------------------------------ ID = 624 ------------------------------\n",
      "ASR question:\t\t which airports do not have departing our arriving flights.\n",
      "Rewritten question:\t Which airports do not have departing have departing flights ?\n",
      "Gold question:\t\t Which airports do not have departing or arriving flights?\n",
      "Rewritten-q Pred SQL:\t SELECT airports.AirportName FROM airports WHERE airports.AirportCode NOT IN (SELECT flights.SourceAirport FROM flights)\n",
      "Gold SQL:\t\t SELECT AirportName FROM Airports WHERE AirportCode NOT IN (SELECT SourceAirport FROM Flights UNION SELECT DestAirport FROM Flights)\n",
      "Score: 0.875\n",
      "------------------------------ ID = 712 ------------------------------\n",
      "ASR question:\t\t shou all template. It's a number of documents using each template.\n",
      "Rewritten question:\t Show all template ids and number of documents using each template .\n",
      "Gold question:\t\t Show all template ids and number of documents using each template.\n",
      "Rewritten-q Pred SQL:\t SELECT Documents.Template_ID, Count(*) FROM Documents GROUP BY Documents.Template_ID\n",
      "Gold SQL:\t\t SELECT template_id ,  count(*) FROM Documents GROUP BY template_id\n",
      "Score: 1.0\n",
      "------------------------------ ID = 800 ------------------------------\n",
      "ASR question:\t\t What are the odds of templates with template type code P P or P Petey?\n",
      "Rewritten question:\t What are the ids of templates with template type with template type code `` Studio '' ?\n",
      "Gold question:\t\t What are the ids of templates with template type code PP or PPT?\n",
      "Rewritten-q Pred SQL:\t SELECT Templates.Template_ID FROM Templates WHERE Templates.Template_Type_Code = 'terminal'\n",
      "Gold SQL:\t\t SELECT template_id FROM Templates WHERE template_type_code  =  \"PP\" OR template_type_code  =  \"PPT\"\n",
      "Score: 0.2222222222222222\n",
      "------------------------------ ID = 888 ------------------------------\n",
      "ASR question:\t\t what is the template type code of the template used by document with the name data base?\n",
      "Rewritten question:\t What is the template type code of the template used by document with the name `` Life Insurance '' ?\n",
      "Gold question:\t\t What is the template type code of the template used by document with the name \"Data base\"?\n",
      "Rewritten-q Pred SQL:\t SELECT Templates.Template_Type_Code FROM Templates JOIN Documents ON Templates.Template_ID = Documents.Template_ID WHERE Documents.Document_Name = 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.template_type_code FROM Templates AS T1 JOIN Documents AS T2 ON T1.template_id  =  T2.template_id WHERE T2.document_name  =  \"Data base\"\n",
      "Score: 0.8333333333333334\n",
      "------------------------------ ID = 976 ------------------------------\n",
      "ASR question:\t\t return the type code of the template type of the description book.\n",
      "Rewritten question:\t Return the type code of the template type of the description `` cumin '' .\n",
      "Gold question:\t\t Return the type code of the template type with the description \"Book\".\n",
      "Rewritten-q Pred SQL:\t SELECT Ref_Template_Types.Template_Type_Code FROM Ref_Template_Types WHERE Ref_Template_Types.Template_Type_Description = 'terminal'\n",
      "Gold SQL:\t\t SELECT template_type_code FROM Ref_template_types WHERE template_type_description  =  \"Book\"\n",
      "Score: 0.8333333333333334\n",
      "------------------------------ ID = 1064 ------------------------------\n",
      "ASR question:\t\t What are the paragraph texts for the document with the name customer reviews.\n",
      "Rewritten question:\t What are the paragraph texts for the document with the name `` Life Insurance '' ?\n",
      "Gold question:\t\t What are the paragraph texts for the document with the name 'Customer reviews'?\n",
      "Rewritten-q Pred SQL:\t SELECT Paragraphs.Paragraph_Text FROM Documents JOIN Paragraphs ON Documents.Document_ID = Paragraphs.Document_ID WHERE Documents.Document_Name = 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.paragraph_text FROM Paragraphs AS T1 JOIN Documents AS T2 ON T1.document_id  =  T2.document_id WHERE T2.document_name  =  \"Customer reviews\"\n",
      "Score: 0.8333333333333334\n",
      "------------------------------ ID = 1152 ------------------------------\n",
      "ASR question:\t\t show the document idea with paragraph texts Brazil in Ireland.\n",
      "Rewritten question:\t Show the document type with paragraph texts Brazil for in Ireland .\n",
      "Gold question:\t\t Show the document id with paragraph text 'Brazil' and 'Ireland'.\n",
      "Rewritten-q Pred SQL:\t SELECT Paragraphs.Document_ID FROM Paragraphs WHERE Paragraphs.Paragraph_Text LIKE 'terminal'\n",
      "Gold SQL:\t\t SELECT document_id FROM Paragraphs WHERE paragraph_text  =  'Brazil' INTERSECT SELECT document_id FROM Paragraphs WHERE paragraph_text  =  'Ireland'\n",
      "Score: 0.5\n",
      "------------------------------ ID = 1240 ------------------------------\n",
      "ASR question:\t\t list the most common hometown of teachers.\n",
      "Rewritten question:\t List the most common grade of teachers .\n",
      "Gold question:\t\t List the most common hometown of teachers.\n",
      "Rewritten-q Pred SQL:\t SELECT course_arrange.Grade FROM course_arrange GROUP BY course_arrange.Grade ORDER BY Count(*) Desc LIMIT 1\n",
      "Gold SQL:\t\t SELECT Hometown FROM teacher GROUP BY Hometown ORDER BY COUNT(*) DESC LIMIT 1\n",
      "Score: 0.6\n",
      "------------------------------ ID = 1328 ------------------------------\n",
      "ASR question:\t\t What are the names of the teachers, and how many courses do they teach you?\n",
      "Rewritten question:\t What are the names of the teachers and how many courses do they teach ?\n",
      "Gold question:\t\t What are the names of the teachers and how many courses do they teach?\n",
      "Rewritten-q Pred SQL:\t SELECT teacher.Name, Count(*) FROM teacher JOIN course_arrange ON teacher.Teacher_ID = course_arrange.Teacher_ID GROUP BY teacher.Teacher_ID\n",
      "Gold SQL:\t\t SELECT T2.Name ,  COUNT(*) FROM course_arrange AS T1 JOIN teacher AS T2 ON T1.Teacher_ID  =  T2.Teacher_ID GROUP BY T2.Name\n",
      "Score: 0.75\n",
      "------------------------------ ID = 1416 ------------------------------\n",
      "ASR question:\t\t what are the idee name and membership level of visitors who have spent the largest amount of money in total in all museum tickets?\n",
      "Rewritten question:\t What are the the id , name and membership level of visitors who have spent the largest amount of money in all museum tickets ?\n",
      "Gold question:\t\t What are the id, name and membership level of visitors who have spent the largest amount of money in total in all museum tickets?\n",
      "Rewritten-q Pred SQL:\t SELECT visitor.ID, visitor.Name, visitor.Level_of_membership FROM museum JOIN visitor JOIN visit ON museum.Museum_ID = visit.Museum_ID AND visit.visitor_ID = visitor.ID GROUP BY visitor.ID ORDER BY Sum(museum.Num_of_Staff) Desc LIMIT 1\n",
      "Gold SQL:\t\t SELECT t2.visitor_id ,  t1.name ,  t1.Level_of_membership FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id GROUP BY t2.visitor_id ORDER BY sum(t2.Total_spent) DESC LIMIT 1\n",
      "Score: 0.35714285714285715\n",
      "------------------------------ ID = 1504 ------------------------------\n",
      "ASR question:\t\t find the average age of losers and winners of all matches.\n",
      "Rewritten question:\t Find the average age of losers and winners of all matches .\n",
      "Gold question:\t\t Find the average age of losers and winners of all matches.\n",
      "Rewritten-q Pred SQL:\t SELECT Avg(matches.loser_age), Avg(matches.loser_age) FROM matches\n",
      "Gold SQL:\t\t SELECT avg(loser_age) ,  avg(winner_age) FROM matches\n",
      "Score: 0.2\n",
      "------------------------------ ID = 1592 ------------------------------\n",
      "ASR question:\t\t what are the first names and country coats for players who won both the W th championships in the Australian Open?\n",
      "Rewritten question:\t What are the first names and country coats for players who won the the W UK Vat championships and the Australian Open ?\n",
      "Gold question:\t\t What are the first names and country codes for players who won both the WTA Championships and the Australian Open?\n",
      "Rewritten-q Pred SQL:\t SELECT players.first_name, players.country_code FROM players JOIN matches ON players.player_id = matches.loser_id WHERE matches.tourney_level = 'terminal' INTERSECT SELECT players.first_name, players.country_code FROM players JOIN matches ON players.player_id = matches.loser_id WHERE matches.tourney_level = 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'WTA Championships' INTERSECT SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'Australian Open'\n",
      "Score: 0.7\n",
      "------------------------------ ID = 1680 ------------------------------\n",
      "ASR question:\t\t find the names of looser and winter who played in the match with greatest number of minutes.\n",
      "Rewritten question:\t Find the names of all channels who played in the match with greatest number of minutes .\n",
      "Gold question:\t\t find the names of loser and winner who played in the match with greatest number of minutes.\n",
      "Rewritten-q Pred SQL:\t SELECT players.first_name FROM players JOIN matches ON players.player_id = matches.loser_id ORDER BY matches.minutes Desc LIMIT 1\n",
      "Gold SQL:\t\t SELECT winner_name ,  loser_name FROM matches ORDER BY minutes DESC LIMIT 1\n",
      "Score: 0.5555555555555556\n",
      "------------------------------ ID = 1768 ------------------------------\n",
      "ASR question:\t\t how many different winners both participated in the W th championships and were left handed.\n",
      "Rewritten question:\t How many different winners both participated in the tourney th and were left handed ?\n",
      "Gold question:\t\t How many different winners both participated in the WTA Championships and were left handed?\n",
      "Rewritten-q Pred SQL:\t SELECT Count(DISTINCT matches.winner_id) FROM matches\n",
      "Gold SQL:\t\t SELECT count(DISTINCT winner_name) FROM matches WHERE tourney_name  =  'WTA Championships' AND winner_hand  =  'L'\n",
      "Score: 0.0\n",
      "------------------------------ ID = 1856 ------------------------------\n",
      "ASR question:\t\t what did the contestant numbers and names of the contestants who had at least two boats?\n",
      "Rewritten question:\t What are the contestant numbers and names of the contestants who had at least 2 boats ?\n",
      "Gold question:\t\t What are the contestant numbers and names of the contestants who had at least two votes?\n",
      "Rewritten-q Pred SQL:\t SELECT CONTESTANTS.contestant_number, CONTESTANTS.contestant_name FROM CONTESTANTS JOIN VOTES ON CONTESTANTS.contestant_number = VOTES.contestant_number GROUP BY VOTES.contestant_number HAVING Count(*) >= 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.contestant_number , T1.contestant_name FROM contestants AS T1 JOIN votes AS T2 ON T1.contestant_number  =  T2.contestant_number GROUP BY T1.contestant_number HAVING count(*)  >=  2\n",
      "Score: 0.8888888888888888\n",
      "------------------------------ ID = 1944 ------------------------------\n",
      "ASR question:\t\t which continent is angrily inn.\n",
      "Rewritten question:\t Which 3 wineries state is Fast As a Shark ?\n",
      "Gold question:\t\t Which continent is Anguilla in?\n",
      "Rewritten-q Pred SQL:\t SELECT country.HeadOfState FROM country WHERE country.GovernmentForm = 'terminal'\n",
      "Gold SQL:\t\t SELECT Continent FROM country WHERE Name  =  \"Anguilla\"\n",
      "Score: 0.3333333333333333\n",
      "------------------------------ ID = 2032 ------------------------------\n",
      "ASR question:\t\t give the total surface area covered by countries in Asia or Europe.\n",
      "Rewritten question:\t Give the total surface area by countries in Asia or or Europe .\n",
      "Gold question:\t\t Give the total surface area covered by countries in Asia or Europe.\n",
      "Rewritten-q Pred SQL:\t SELECT Sum(country.SurfaceArea) FROM country WHERE country.Continent = 'terminal' OR country.Continent = 'terminal'\n",
      "Gold SQL:\t\t SELECT sum(SurfaceArea) FROM country WHERE Continent  =  \"Asia\" OR Continent  =  \"Europe\"\n",
      "Score: 0.7777777777777778\n",
      "------------------------------ ID = 2120 ------------------------------\n",
      "ASR question:\t\t what are the names of nations? Speak both English and French?\n",
      "Rewritten question:\t What are the names of nations Bootup English and French ?\n",
      "Gold question:\t\t What are the names of nations speak both English and French?\n",
      "Rewritten-q Pred SQL:\t SELECT country.Name FROM country JOIN countrylanguage ON country.Code = countrylanguage.CountryCode WHERE countrylanguage.Language = 'terminal' AND countrylanguage.Language = 'terminal'\n",
      "Gold SQL:\t\t SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\"\n",
      "Score: 0.2\n",
      "------------------------------ ID = 2208 ------------------------------\n",
      "ASR question:\t\t what languages are only used by a single country with the Republic government.\n",
      "Rewritten question:\t What languages are only used by a single country with the Republic government ?\n",
      "Gold question:\t\t What languages are only used by a single country with a republic government?\n",
      "Rewritten-q Pred SQL:\t SELECT countrylanguage.Language FROM country JOIN countrylanguage ON country.Code = countrylanguage.CountryCode WHERE country.GovernmentForm = 'terminal'\n",
      "Gold SQL:\t\t SELECT T2.Language FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T1.GovernmentForm  =  \"Republic\" GROUP BY T2.Language HAVING COUNT(*)  =  1\n",
      "Score: 0.4\n",
      "------------------------------ ID = 2296 ------------------------------\n",
      "ASR question:\t\t what are the country code is for countries that do not speak English.\n",
      "Rewritten question:\t What are the country names for countries that do not speak English ?\n",
      "Gold question:\t\t What are the country codes for countries that do not speak English?\n",
      "Rewritten-q Pred SQL:\t SELECT country.Name FROM country EXCEPT SELECT country.Name FROM country JOIN countrylanguage ON country.Code = countrylanguage.CountryCode WHERE countrylanguage.Language = 'terminal'\n",
      "Gold SQL:\t\t SELECT CountryCode FROM countrylanguage EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "Score: 0.4\n",
      "------------------------------ ID = 2384 ------------------------------\n",
      "ASR question:\t\t return the country name, and the numbers of language is spoken for each country that speaks at least three languages.\n",
      "Rewritten question:\t Return the country name and the numbers of cinemas spoken for each country that speaks at least three languages .\n",
      "Gold question:\t\t Return the country name and the numbers of languages spoken for each country that speaks at least 3 languages.\n",
      "Rewritten-q Pred SQL:\t SELECT country.Name, Count(*) FROM city JOIN country ON city.CountryCode = country.Code GROUP BY city.CountryCode HAVING Count(*) >= 'terminal'\n",
      "Gold SQL:\t\t SELECT COUNT(T2.Language) ,  T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name HAVING COUNT(*)  >  2\n",
      "Score: 0.3333333333333333\n",
      "------------------------------ ID = 2472 ------------------------------\n",
      "ASR question:\t\t gave the names of countries that are in Europe and have a population equal to 80,000.\n",
      "Rewritten question:\t Give the names of countries that are in Europe and have a population equal to countries .\n",
      "Gold question:\t\t Give the names of countries that are in Europe and have a population equal to 80000.\n",
      "Rewritten-q Pred SQL:\t SELECT country.Name FROM country WHERE country.Region = 'terminal' AND country.Population = 'terminal'\n",
      "Gold SQL:\t\t SELECT Name FROM country WHERE continent  =  \"Europe\" AND Population  =  \"80000\"\n",
      "Score: 0.5\n",
      "------------------------------ ID = 2560 ------------------------------\n",
      "ASR question:\t\t return the names of conductors that do not have the nationality U. S A.\n",
      "Rewritten question:\t Return the names of conductors that do not have the nationality `` Village '' .\n",
      "Gold question:\t\t Return the names of conductors that do not have the nationality \"USA\".\n",
      "Rewritten-q Pred SQL:\t SELECT conductor.Name FROM conductor WHERE conductor.Nationality != 'terminal'\n",
      "Gold SQL:\t\t SELECT Name FROM conductor WHERE Nationality != 'USA'\n",
      "Score: 0.8333333333333334\n",
      "------------------------------ ID = 2648 ------------------------------\n",
      "ASR question:\t\t what is the name of the conductor who has conducted the most to orchestras?\n",
      "Rewritten question:\t What is the name of the conductor who has conducted the most to orchestras ?\n",
      "Gold question:\t\t What is the name of the conductor who has conducted the most orchestras?\n",
      "Rewritten-q Pred SQL:\t SELECT conductor.Name FROM conductor JOIN orchestra ON conductor.Conductor_ID = orchestra.Conductor_ID GROUP BY orchestra.Conductor_ID ORDER BY Count(*) Desc LIMIT 1\n",
      "Gold SQL:\t\t SELECT T1.Name FROM conductor AS T1 JOIN orchestra AS T2 ON T1.Conductor_ID  =  T2.Conductor_ID GROUP BY T2.Conductor_ID ORDER BY COUNT(*) DESC LIMIT 1\n",
      "Score: 1.0\n",
      "------------------------------ ID = 2736 ------------------------------\n",
      "ASR question:\t\t show the years in which orchestra's that have given more than one performance are founded.\n",
      "Rewritten question:\t Show the years in which parties that have given more than one performance are founded .\n",
      "Gold question:\t\t Show the years in which orchestras that have given more than one performance are founded.\n",
      "Rewritten-q Pred SQL:\t SELECT orchestra.Year_of_Founded FROM orchestra JOIN performance ON orchestra.Orchestra_ID = performance.Orchestra_ID GROUP BY orchestra.Year_of_Founded HAVING Count(*) > 'terminal'\n",
      "Gold SQL:\t\t SELECT Year_of_Founded FROM orchestra AS T1 JOIN performance AS T2 ON T1.Orchestra_ID  =  T2.Orchestra_ID GROUP BY T2.Orchestra_ID HAVING COUNT(*)  >  1\n",
      "Score: 0.7142857142857143\n",
      "------------------------------ ID = 2824 ------------------------------\n",
      "ASR question:\t\t how many high schoolers air there, ingrate. Nine or 10.\n",
      "Rewritten question:\t How many high schoolers are there at `` AKW '' or 10 ?\n",
      "Gold question:\t\t How many high schoolers are there in grade 9 or 10?\n",
      "Rewritten-q Pred SQL:\t SELECT Count(*) FROM Highschooler WHERE Highschooler.grade = 'terminal' OR Highschooler.grade = 'terminal'\n",
      "Gold SQL:\t\t SELECT count(*) FROM Highschooler WHERE grade  =  9 OR grade  =  10\n",
      "Score: 0.7777777777777778\n",
      "------------------------------ ID = 2912 ------------------------------\n",
      "ASR question:\t\t returned the names of friends of a high school student, Kyle.\n",
      "Rewritten question:\t Return the names of friends of no high school student .\n",
      "Gold question:\t\t Return the names of friends of the high school student Kyle.\n",
      "Rewritten-q Pred SQL:\t SELECT Highschooler.name FROM Highschooler WHERE Highschooler.ID NOT IN (SELECT Friend.friend_id FROM Friend)\n",
      "Gold SQL:\t\t SELECT T3.name FROM Friend AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id JOIN Highschooler AS T3 ON T1.friend_id  =  T3.id WHERE T2.name  =  \"Kyle\"\n",
      "Score: 0.375\n",
      "------------------------------ ID = 3000 ------------------------------\n",
      "ASR question:\t\t whatever names of high schoolers who have likes and how many likes does each have.\n",
      "Rewritten question:\t What are the names of high schoolers who have likes and how many likes does each have ?\n",
      "Gold question:\t\t What are the names of high schoolers who have likes, and how many likes does each have?\n",
      "Rewritten-q Pred SQL:\t SELECT Highschooler.name, Count(*) FROM Highschooler JOIN Likes ON Highschooler.ID = Likes.liked_id GROUP BY Likes.student_id\n",
      "Gold SQL:\t\t SELECT T2.name ,  count(*) FROM Likes AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id GROUP BY T1.student_id\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_samples = [c for d in test_dataset for c in d]\n",
    "for i, c in list(enumerate(test_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['query'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_ILM_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dev_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite_seq postprocessing, to get the rewritten question \n",
    "\n",
    "_idx = 154\n",
    "\n",
    "_tags = rewriter_ILM_preds[_idx]['tags_prediction']\n",
    "_rewrite_seq = rewriter_ILM_preds[_idx]['rewrite_seq_prediction']\n",
    "_question_toks = rewriter_ILM_preds[_idx]['question'].split(' ')\n",
    "_tags, _rewrite_seq, _question_toks\n",
    "\n",
    "postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'concert_singer'\n",
    "g_str = 'SELECT count(*) FROM singer'\n",
    "p_str = \"SELECT Count(DISTINCT singer.Name) FROM singer WHERE singer.Name = 'terminal'\"\n",
    "\n",
    "db, p_str, g_str, EvaluateSQL(p_str, g_str, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
