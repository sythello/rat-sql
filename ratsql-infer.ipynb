{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "from sys import modules\n",
    "import _jsonnet\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "import editdistance\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/third_party/wikisql'))\n",
    "# sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.datasets.spider import SpiderItem\n",
    "from ratsql.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "from SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation\n",
    "from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, \\\n",
    "    Postprocess_rewrite_seq, Postprocess_rewrite_seq_freeze_POS, Postprocess_rewrite_seq_modify_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeakQL.Allennlp_models.utils import misc_utils\n",
    "importlib.reload(misc_utils)\n",
    "from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, \\\n",
    "    Postprocess_rewrite_seq, Postprocess_rewrite_seq_freeze_POS, Postprocess_rewrite_seq_modify_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modules['SpeakQL.Allennlp_models.utils.misc_utils']\n",
    "# del EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq\n",
    "# from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rat-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Rat_sql(root_dir,\n",
    "                 exp_config_path,\n",
    "                 model_dir,\n",
    "                 checkpoint_step=40000):\n",
    "\n",
    "    exp_config = json.loads(_jsonnet.evaluate_file(exp_config_path))\n",
    "    \n",
    "    model_config_path = os.path.join(root_dir, exp_config[\"model_config\"])\n",
    "    model_config_args = exp_config.get(\"model_config_args\")\n",
    "    \n",
    "    infer_config = json.loads(_jsonnet.evaluate_file(model_config_path, tla_codes={'args': json.dumps(model_config_args)}))\n",
    "\n",
    "    inferer = Inferer(infer_config)\n",
    "    inferer.device = torch.device(\"cpu\")\n",
    "    model = inferer.load_model(model_dir, checkpoint_step)\n",
    "    dataset = registry.construct('dataset', inferer.config['data']['val'])\n",
    "\n",
    "    for _, schema in dataset.schemas.items():\n",
    "        model.preproc.enc_preproc._preprocess_schema(schema)\n",
    "    \n",
    "    _ret_dict = {\n",
    "        'model': model,\n",
    "        'dataset': dataset,\n",
    "        'inferer': inferer,\n",
    "    }\n",
    "    \n",
    "    return _ret_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0/model_checkpoint-00040000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|██████████| 166/166 [00:02<00:00, 79.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dataset', 'inferer'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_sql_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                  exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-run.jsonnet',\n",
    "                                  model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                  checkpoint_step=40000)\n",
    "rat_sql_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_sql_asr_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                      exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-ASR-run.jsonnet',\n",
    "                                      model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_ASR_run/ASR,bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                      checkpoint_step=40000)\n",
    "rat_sql_asr_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_sql_mixed_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                        exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-mixed-run.jsonnet',\n",
    "                                        model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_mixed_run/mixed,bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                        checkpoint_step=40000)\n",
    "rat_sql_mixed_model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dicts = {\n",
    "    'orig': rat_sql_model_dict,\n",
    "#     'asr': rat_sql_asr_model_dict,\n",
    "#     'mixed': rat_sql_mixed_model_dict,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question(q, db_id, model_dict=model_dicts['orig']):\n",
    "    model = model_dict['model']\n",
    "    dataset = model_dict['dataset']\n",
    "    inferer = model_dict['inferer']\n",
    "    \n",
    "    spider_schema = dataset.schemas[db_id]\n",
    "    data_item = SpiderItem(\n",
    "        text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly\n",
    "        code=None,\n",
    "        schema=spider_schema,\n",
    "        orig_schema=spider_schema.orig,\n",
    "        orig={\"question\": q}\n",
    "    )\n",
    "    model.preproc.clear_items()\n",
    "    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)\n",
    "    preproc_data = enc_input, None\n",
    "    with torch.no_grad():\n",
    "        return inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many singers do we have?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where'},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 1}]}},\n",
       "  'inferred_code': 'SELECT Count(*) FROM singer',\n",
       "  'score': -0.00011539317730324683}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many singers have held at least 2 concerts?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where'},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 3}]}},\n",
       "  'inferred_code': 'SELECT Count(*) FROM singer_in_concert',\n",
       "  'score': -0.5656531027781071}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many singers have held at least 2 concerts?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many singers have held at least 2 concerts ?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where'},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 3}]}},\n",
       "  'inferred_code': 'SELECT Count(*) FROM singer_in_concert',\n",
       "  'score': -0.5656531027781071}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many singers have held at least 2 concerts ?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many airlines come from from CA?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where',\n",
       "    'where': {'_type': 'Eq',\n",
       "     'val_unit': {'_type': 'Column',\n",
       "      'col_unit1': {'_type': 'col_unit',\n",
       "       'agg_id': {'_type': 'NoneAggOp'},\n",
       "       'col_id': 3,\n",
       "       'is_distinct': False}},\n",
       "     'val1': {'_type': 'Terminal'}}},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 0}]}},\n",
       "  'inferred_code': \"SELECT Count(*) FROM airlines WHERE airlines.Abbreviation = 'terminal'\",\n",
       "  'score': -0.012203325939928789}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many airlines come from from CA?\", \"flight_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\", model_dict=model_dicts['mixed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\",\n",
    "         model_dict=model_dicts['mixed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_path, 'r') as f:\n",
    "    dev_dataset = json.load(f)\n",
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dev_dataset[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(d['question'], d['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sql_list = []\n",
    "\n",
    "for d in tqdm(dev_dataset):\n",
    "    pred = question(d['question'], d['db_id'])[0]\n",
    "    pred_sql_list.append(pred['inferred_code'])\n",
    "\n",
    "len(pred_sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_path = './output/dev_output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dev_pred_path, 'w') as f:\n",
    "#     for pred in pred_sql_list:\n",
    "#         f.write(pred + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict SQL given transcribed dataset JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'train'\n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon.json'.format(DATASET)\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon_RatsqlPredicted.json'.format(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for human test \n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon.json'\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon_RatsqlPredicted.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_dataset_path, 'r') as f:\n",
    "    asr_dataset = json.load(f)\n",
    "len(asr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, d in tqdm(enumerate(asr_dataset), total=len(asr_dataset)):\n",
    "    if 'ratsql_pred_sql' in d:\n",
    "        continue\n",
    "        \n",
    "    pred = Question(d['question'], d['db_id'])\n",
    "    if len(pred) == 0:\n",
    "        print('{}: question({}, {}) failed'.format(i, d['question'], d['db_id']))\n",
    "        d['ratsql_pred_sql'] = ''\n",
    "    else:\n",
    "        d['ratsql_pred_sql'] = pred[0]['inferred_code']\n",
    "\n",
    "asr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = question(d['question'], d['db_id'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_dataset_path, 'w') as f:\n",
    "#     json.dump(asr_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict & Evaluate SQL given rewriter output / original / ASR cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1.0, 0), (0, 0.5, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateSQL(pred_str='SELECT * FROM singer WHERE name = \"Joe Sharp\"',\n",
    "            gold_str='SELECT * FROM singer WHERE name = \"DEF\"',\n",
    "            db='concert_singer'), \\\n",
    "EvaluateSQL(pred_str='SELECT country FROM singer WHERE name = \"ABC\"',\n",
    "            gold_str='SELECT country FROM singer WHERE name = \"DEF\" ORDER BY age LIMIT 1',\n",
    "            db='concert_singer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset_clean = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset_clean = json.load(f)\n",
    "\n",
    "len(test_dataset_clean), sum([len(d) for d in test_dataset_clean]), len(orig_dev_dataset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black box (first cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # human test \n",
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "# orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2ec7fc725144d9976beb8a7feadbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Just using the 1st ASR candidate, no correction \n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "wer_numer = 0\n",
    "wer_denom = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "#     _pred_sql = Question(c['question'], _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "    \n",
    "#     _gold_sql = c['query']\n",
    "#     _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "#     c['pred_sql'] = _pred_sql\n",
    "#     c['score'] = _score\n",
    "#     c['exact'] = _exact\n",
    "#     c['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n",
    "    wer_numer += editdistance.eval(_gold_question_toks, _question_toks)\n",
    "    wer_denom += len(_gold_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 0.8010\n",
      "WER = 0.1194\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "# _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "# _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "# _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "_wer = 1.0 * wer_numer / (wer_denom + 1e-9)\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "# print('avg = {:.4f}'.format(_avg_1st))\n",
    "# print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "# print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n",
    "print(f'WER = {_wer:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853, 7146)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_numer, wer_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the predictions \n",
    "# Orig\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/Assembly/First-cands.json'\n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/Assembly/First-cands.json'\n",
    "# Mixed\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/Assembly/First-cands.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold (original text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a4a806790a4b25801a23815824e379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['gold_question'], _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.8316\n",
      "avg_exact = 0.6234\n",
      "avg_exec = 0.4095\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_subset = random.sample(test_dataset, k=100)\n",
    "\n",
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in random_subset],\n",
    "                 glist=[d[0]['query'] for d in random_subset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in random_subset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postprocess_rewrite_seq_heuristics(cand_dict, pred_dict=None):\n",
    "    '''\n",
    "    Heuristics: (not using preds)\n",
    "    [punct] it's [punct] -> ids\n",
    "    i d -> id\n",
    "    idea -> id\n",
    "    odds -> ids\n",
    "    [punct] id [punct] -> id\n",
    "    [punct] ids [punct] -> ids\n",
    "    \"Whxx/How\": ending \".\" -> \"?\"\n",
    "    number: text -> digits\n",
    "    coats -> codes\n",
    "    '''\n",
    "    \n",
    "    Num_dict = {\n",
    "        \"one\": \"1\",\n",
    "        \"two\": \"2\",\n",
    "        \"three\": \"3\",\n",
    "        \"four\": \"4\",\n",
    "        \"five\": \"5\",\n",
    "        \"six\": \"6\",\n",
    "        \"seven\": \"7\",\n",
    "        \"eight\": \"8\",\n",
    "        \"nine\": \"9\",\n",
    "        \"ten\": \"10\",\n",
    "        \"eleven\": \"11\",\n",
    "        \"twelve\": \"12\",\n",
    "        \"thirteen\": \"13\",\n",
    "        \"fourteen\": \"14\",\n",
    "        \"fifteen\": \"15\",\n",
    "        \"sixteen\": \"16\",\n",
    "        \"seventeen\": \"17\",\n",
    "        \"eighteen\": \"18\",\n",
    "        \"nineteen\": \"19\",\n",
    "        \"twenty\": \"20\",\n",
    "    }\n",
    "    \n",
    "    question_toks = [\"[PAD]\"] + [t.lower() for t in cand_dict['question_toks']] + [\"[PAD]\"]\n",
    "    rewritten_toks = []\n",
    "    \n",
    "    ptr = 1\n",
    "    while ptr < len(question_toks) - 1:\n",
    "        _tok = question_toks[ptr]\n",
    "        _o_tok = _tok\n",
    "        if _tok in {\"it's\", \"ids\", \"id\"}:\n",
    "            if _tok == \"it's\":\n",
    "                _o_tok = \"ids\"\n",
    "            # other (ids, id) unchanged \n",
    "            \n",
    "            # remove surronding puncts\n",
    "            if question_toks[ptr - 1] in '.,?':\n",
    "                rewritten_toks = rewritten_toks[:-1]\n",
    "            if question_toks[ptr + 1] in '.,?':\n",
    "                ptr += 1\n",
    "        elif _tok == 'odds':\n",
    "            _o_tok = 'ids'\n",
    "        elif _tok == 'idea':\n",
    "            _o_tok = 'id'\n",
    "        elif _tok in Num_dict:\n",
    "            _o_tok = Num_dict[_tok]\n",
    "        elif _tok == 'coats':\n",
    "            _o_tok == 'codes'\n",
    "#         elif ptr == len(question_toks) - 2 and _tok in '.?':\n",
    "#             if question_toks[1].startswith('how') or question_toks[1].startswith('wh'):\n",
    "#                 _o_tok = '?'\n",
    "#             else:\n",
    "#                 _o_tok = '.'\n",
    "        rewritten_toks.append(_o_tok)\n",
    "        ptr += 1\n",
    "        \n",
    "    if rewritten_toks[-1] not in '.?':\n",
    "        rewritten_toks.append('.')\n",
    "    if question_toks[1].startswith('how') or question_toks[1].startswith('wh'):\n",
    "        rewritten_toks[-1] = '?'\n",
    "    else:\n",
    "        rewritten_toks[-1] = '.'\n",
    "        \n",
    "    return rewritten_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = deepcopy(test_dataset_clean)\n",
    "orig_dev_dataset = deepcopy(orig_dev_dataset_clean)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f07eaad996a4f1ea0de67205da28d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _rewritten_question_toks = Postprocess_rewrite_seq_heuristics(c, None)\n",
    "    _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_rewritten_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7481\n",
      "avg_exact = 0.4899\n",
      "avg_exec = 0.3510\n",
      "BLEU = 0.8306\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "    \n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRIDGE results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TabularSemanticParsing/server/spider.bridge.lstm.meta.ts.ppl-0.85.2.dn.eo.feat.bert-base-uncased.xavier-768-400-400-8-4-0.0005-inv-sqr-0.0005-4000-6e-05-inv-sqr-3e-05-4000-0.1-0.3-0.0-0.0-1-8-0.0-0.0-res-0.2-0.0-ff-0.4-0.0.210126-135032.7kf9/predictions.16.ASR.dev.txt'\n",
    "gold_query_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev_gold.sql'\n",
    "test_index_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/index_test.txt'\n",
    "# db_id_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/db_id.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1034, 1034, 547)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(bridge_pred_path, 'r') as f:\n",
    "    bridge_preds = f.read().strip().split('\\n')\n",
    "with open(gold_query_path, 'r') as f:\n",
    "    _lines = [l.split('\\t') for l in f.read().strip().split('\\n')]\n",
    "    gold_queries = [l[0] for l in _lines]\n",
    "    db_ids = [l[1] for l in _lines]\n",
    "# with open(db_id_path, 'r') as f:\n",
    "#     db_ids = f.read().strip().split('\\n')\n",
    "with open(test_index_path, 'r') as f:\n",
    "    test_indices = [int(i) for i in f.read().strip().split('\\n')]\n",
    "    \n",
    "len(bridge_preds), len(gold_queries), len(db_ids), len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e1b7f2bde42629b9834007d10e231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "eval_err_num:2\n",
      "eval_err_num:3\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.654                0.442                0.451                0.325                0.479               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.750                0.450                0.429                0.287                0.497               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.882                0.679                0.824                0.762                0.767               \n",
      "select(no AGG)       0.904                0.688                0.824                0.762                0.776               \n",
      "where                0.812                0.626                0.609                0.413                0.631               \n",
      "where(no OP)         0.859                0.652                0.652                0.500                0.675               \n",
      "group(no Having)     0.625                0.788                0.680                0.781                0.748               \n",
      "group                0.625                0.727                0.680                0.750                0.712               \n",
      "order                0.889                0.579                0.828                0.886                0.766               \n",
      "and/or               1.000                0.950                0.956                0.872                0.952               \n",
      "IUEN                 0.000                0.000                0.579                0.538                0.529               \n",
      "keywords             0.899                0.885                0.837                0.772                0.860               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.882                0.671                0.824                0.762                0.762               \n",
      "select(no AGG)       0.904                0.679                0.824                0.762                0.771               \n",
      "where                0.800                0.643                0.560                0.396                0.622               \n",
      "where(no OP)         0.846                0.670                0.600                0.479                0.665               \n",
      "group(no Having)     0.625                0.684                0.773                0.658                0.684               \n",
      "group                0.625                0.632                0.773                0.632                0.651               \n",
      "order                1.000                0.611                0.774                0.816                0.752               \n",
      "and/or               0.993                0.991                1.000                0.944                0.987               \n",
      "IUEN                 0.000                0.000                0.458                0.438                0.450               \n",
      "keywords             0.899                0.864                0.791                0.762                0.840               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.882                0.675                0.824                0.762                0.764               \n",
      "select(no AGG)       0.904                0.683                0.824                0.762                0.774               \n",
      "where                0.806                0.634                0.583                0.404                0.626               \n",
      "where(no OP)         0.853                0.661                0.625                0.489                0.670               \n",
      "group(no Having)     0.625                0.732                0.723                0.714                0.715               \n",
      "group                0.625                0.676                0.723                0.686                0.680               \n",
      "order                0.941                0.595                0.800                0.849                0.759               \n",
      "and/or               0.996                0.970                0.978                0.907                0.969               \n",
      "IUEN                 1.000                1.000                0.512                0.483                0.486               \n",
      "keywords             0.899                0.875                0.814                0.767                0.850               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.884                0.708                0.735                0.683                0.753               \n",
      "\n",
      "Exact: 0.75\t0.45\t0.42857142857142855\t0.2875\t0.49725776965265084\n",
      "Exec: 0.6544117647058824\t0.44166666666666665\t0.45054945054945056\t0.325\t0.4789762340036563\n",
      "Partial: 0.8836285650623887\t0.7084778300403303\t0.735301023762562\t0.68322823010323\t0.7527949097418934\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=[bridge_preds[idx] for idx in test_indices],\n",
    "                 glist=[gold_queries[idx] for idx in test_indices],\n",
    "                 db_id_list=[db_ids[idx] for idx in test_indices],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80340cac1d7c4ab38c4b78dd31897ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1034.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "eval_err_num:2\n",
      "eval_err_num:3\n",
      "eval_err_num:4\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                250                  440                  174                  170                  1034                \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.652                0.439                0.529                0.276                0.479               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.748                0.468                0.500                0.253                0.506               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.871                0.666                0.851                0.612                0.738               \n",
      "select(no AGG)       0.900                0.670                0.862                0.612                0.749               \n",
      "where                0.821                0.651                0.619                0.384                0.627               \n",
      "where(no OP)         0.868                0.677                0.655                0.475                0.674               \n",
      "group(no Having)     0.650                0.699                0.714                0.742                0.709               \n",
      "group                0.650                0.650                0.714                0.727                0.681               \n",
      "order                0.741                0.641                0.865                0.797                0.754               \n",
      "and/or               1.000                0.959                0.971                0.896                0.961               \n",
      "IUEN                 0.000                0.000                0.581                0.391                0.447               \n",
      "keywords             0.875                0.862                0.839                0.661                0.821               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.868                0.661                0.851                0.612                0.735               \n",
      "select(no AGG)       0.896                0.666                0.862                0.612                0.746               \n",
      "where                0.806                0.680                0.565                0.388                0.626               \n",
      "where(no OP)         0.852                0.708                0.598                0.480                0.672               \n",
      "group(no Having)     0.650                0.656                0.769                0.620                0.662               \n",
      "group                0.650                0.611                0.769                0.608                0.636               \n",
      "order                0.909                0.667                0.763                0.778                0.751               \n",
      "and/or               0.992                0.986                0.994                0.930                0.980               \n",
      "IUEN                 0.000                0.000                0.595                0.250                0.436               \n",
      "keywords             0.887                0.855                0.810                0.641                0.809               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.870                0.664                0.851                0.612                0.736               \n",
      "select(no AGG)       0.898                0.668                0.862                0.612                0.747               \n",
      "where                0.813                0.665                0.591                0.386                0.627               \n",
      "where(no OP)         0.860                0.692                0.625                0.477                0.673               \n",
      "group(no Having)     0.650                0.677                0.741                0.676                0.685               \n",
      "group                0.650                0.630                0.741                0.662                0.658               \n",
      "order                0.816                0.654                0.811                0.787                0.753               \n",
      "and/or               0.996                0.972                0.982                0.913                0.970               \n",
      "IUEN                 1.000                1.000                0.588                0.305                0.442               \n",
      "keywords             0.881                0.858                0.825                0.651                0.815               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.876                0.698                0.767                0.591                0.735               \n",
      "\n",
      "Exact: 0.748\t0.4681818181818182\t0.5\t0.2529411764705882\t0.5058027079303675\n",
      "Exec: 0.652\t0.43863636363636366\t0.5287356321839081\t0.27647058823529413\t0.4787234042553192\n",
      "Partial: 0.8756383838383834\t0.6980433896911165\t0.7674289152737428\t0.5908789419012966\t0.7350393992306947\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=bridge_preds,\n",
    "                 glist=gold_queries,\n",
    "                 db_id_list=db_ids,\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End2end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '4.1.0.1_laststep'\n",
    "\n",
    "end2end_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 1034)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(end2end_pred_path, 'r') as f:\n",
    "    end2end_preds = [json.loads(l) for l in f]\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "    \n",
    "len(end2end_preds), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2067b6b7c9d41dfafda2172d32653bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3075), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "_seen_ids = set()\n",
    "_first_cand_preds = []\n",
    "\n",
    "for i, p in tqdm(enumerate(end2end_preds), total=len(end2end_preds)):\n",
    "\n",
    "    # p = rewriter_ILM_preds[pred_idx]\n",
    "    # _o_idx = c['original_id']\n",
    "    # o = orig_dev_dataset[_o_idx]\n",
    "    # assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    # assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _o_idx = p['original_id']\n",
    "    if _o_idx in _seen_ids:\n",
    "        continue\n",
    "    else:\n",
    "        _seen_ids.add(_o_idx)\n",
    "        \n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "    _pred_sql = p['pred_sql']\n",
    "    _gold_sql = p['gold_sql']\n",
    "    assert _gold_sql == o['query'], (_gold_sql, o['query'])\n",
    "    \n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    # Save prediction results \n",
    "    p['score'] = _score\n",
    "    p['exact'] = _exact\n",
    "    p['exec'] = _exec\n",
    "    \n",
    "    _first_cand_preds.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7660\n",
      "avg_exact = 0.4973\n",
      "avg_exec = 0.3620\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([p['score'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exact_1st = sum([p['exact'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exec_1st = sum([p['exec'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '1.14.1.2'\n",
    "HUMAN_TEST = False\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggreg with rewriter cands \n",
    "\n",
    "RERANKER_VERSION = '1.10.0.2'\n",
    "REWRITER_VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "HUMAN_TEST = True\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/test_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-humantest-yshao-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/human_test_yshao_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(reranker_pred_path, 'r') as f:\n",
    "    reranker_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(reranker_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['score_preds', 'question', 'original_id']),\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance']))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_preds[0].keys(), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9994f50f0dc74926b9ca1cc60bcfb1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Find the cand with highest score \n",
    "    d_preds = []\n",
    "    for _ in d:\n",
    "        p = reranker_preds[pred_idx]\n",
    "        d_preds.append(p)\n",
    "        pred_idx += 1\n",
    "    \n",
    "    _c_idx = np.argmax([p['score_preds'] for p in d_preds])\n",
    "    \n",
    "    c = d[_c_idx]\n",
    "    p = d_preds[_c_idx]\n",
    "    \n",
    "    for _c in d:\n",
    "        _c['is_reranker_selection'] = False\n",
    "    c['is_reranker_selection'] = True\n",
    "    \n",
    "    # Use the selected cand to proceed \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    _question = c['question']\n",
    "\n",
    "    if c['ratsql_pred_sql'] is not None:\n",
    "        # If SQL is already predicted, no need to predict again \n",
    "        _pred_sql = c['ratsql_pred_sql']\n",
    "    else:\n",
    "        # If SQL not yet predicted, do it \n",
    "        _pred_sql = Question(_question, _db_id)[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "#     # Save the taggerILM raw outputs, for later aggregation \n",
    "#     c['pred_tags'] = p['rewriter_tags']\n",
    "#     c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "#     c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "    \n",
    "    # Save prediction results \n",
    "    # c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    # p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "    c['exec'] = p['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7599 (std = 0.2977)\n",
      "avg_exact = 0.5027\n",
      "avg_exec = 0.3620\n",
      "BLEU = 0.7853\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([c['exact'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([c['exec'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[c['pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection']],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non ASR\n",
    "if not HUMAN_TEST:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "else:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "    \n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION: 2.16.0.0t\n",
      "Overall tagger accuracy = 38814/41776 = 0.9291\n",
      "Tagger P = 0.7764, R = 0.6817, F1 = 0.7260\n",
      "\n",
      "VERSION: 2.16.0.1t\n",
      "Overall tagger accuracy = 38782/41776 = 0.9283\n",
      "Tagger P = 0.7529, R = 0.6967, F1 = 0.7237\n",
      "\n",
      "VERSION: 2.16.0.2t\n",
      "Overall tagger accuracy = 38742/41776 = 0.9274\n",
      "Tagger P = 0.7446, R = 0.6858, F1 = 0.7140\n",
      "\n",
      "VERSION: 2.16.0.3t\n",
      "Overall tagger accuracy = 38893/41776 = 0.9310\n",
      "Tagger P = 0.7663, R = 0.6974, F1 = 0.7303\n",
      "\n",
      "VERSION: 2.16.0.4t\n",
      "Overall tagger accuracy = 38932/41776 = 0.9319\n",
      "Tagger P = 0.7640, R = 0.7072, F1 = 0.7345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tagger accuracy \n",
    "VERSION_LIST = [f'2.16.0.{v}t' for v in range(5)]\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    predict_fname = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "\n",
    "    with open(predict_fname, 'r') as f:\n",
    "        predicts = [json.loads(l) for l in f]\n",
    "    len(predicts)\n",
    "\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "\n",
    "    non_O_matches = 0\n",
    "    non_O_preds = 0\n",
    "    non_O_golds = 0\n",
    "\n",
    "    for p in predicts:\n",
    "        _pred_tags = p['tags_prediction']\n",
    "        _gold_tags = [t for t in p['gold_tags'] if t != 'O']\n",
    "        assert len(_pred_tags) == len(_gold_tags)\n",
    "\n",
    "        _cor = sum([_p == _g for _p, _g in zip(_pred_tags, _gold_tags)])\n",
    "        _total = len(_pred_tags)\n",
    "        assert -1e-8 < _cor / _total - p['tags_accuracy'] < 1e-8, f\"{_cor / _total}, {p['tags_accuracy']}\"\n",
    "\n",
    "        for _p, _g in zip(_pred_tags, _gold_tags):\n",
    "            if _p != 'O-KEEP':\n",
    "                non_O_preds += 1\n",
    "            if _g != 'O-KEEP':\n",
    "                non_O_golds += 1\n",
    "            if _p == _g and _p != 'O-KEEP':\n",
    "                non_O_matches += 1\n",
    "\n",
    "        correct_tags += _cor\n",
    "        total_tags += _total\n",
    "\n",
    "    tagger_prec = non_O_matches / non_O_preds\n",
    "    tagger_recall = non_O_matches / non_O_golds\n",
    "    tagger_F1 = 2 * tagger_prec * tagger_recall / (tagger_prec + tagger_recall)\n",
    "\n",
    "    print(f'VERSION: {VERSION}')\n",
    "    print(f'Overall tagger accuracy = {correct_tags}/{total_tags} = {correct_tags / total_tags:.4f}')\n",
    "    print(f'Tagger P = {tagger_prec:.4f}, R = {tagger_recall:.4f}, F1 = {tagger_F1:.4f}')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Postprocess_rewrite_seq_wrapper(cand_dict, pred_dict):\n",
    "    _tags = pred_dict['rewriter_tags']\n",
    "    _rewrite_seq = pred_dict['rewrite_seq_prediction']\n",
    "    _question_toks = cand_dict['question_toks']\n",
    "    return Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_evaluate_ILM(eval_version,\n",
    "                      rewriter_ILM_pred_path,\n",
    "                      test_dataset_path,\n",
    "                      orig_dev_path,\n",
    "                      test_output_path=None,\n",
    "                      rat_sql_run='orig',\n",
    "                      ILM_rewrite_func=_Postprocess_rewrite_seq_wrapper):\n",
    "    \n",
    "    '''\n",
    "    eval_version: simply for printing results \n",
    "    \n",
    "    Example paths:\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    \n",
    "    ILM_rewrite_func: Callable, args: (_tags, _rewrite_seq, _question_toks)\n",
    "    '''\n",
    "    \n",
    "    VERSION = eval_version\n",
    "    \n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}'):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        # Debug \n",
    "        # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "        # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "        _tags = p['rewriter_tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question_toks = ILM_rewrite_func(c, p)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts[rat_sql_run])[0]['inferred_code']\n",
    "\n",
    "        _gold_sql = c['query']\n",
    "        _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        # Save the taggerILM raw outputs, for later aggregation \n",
    "        c['pred_tags'] = p['rewriter_tags']\n",
    "        c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "        c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "        # Save prediction results \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "\n",
    "        _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "\n",
    "    print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "    print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "    # print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "    print('avg = {:.4f}'.format(_avg_1st))\n",
    "    print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print('='*55)\n",
    "    \n",
    "    if test_output_path is not None:\n",
    "        with open(test_output_path, 'w') as f:\n",
    "            json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full tagger-ILM\n",
    "\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Old version\n",
    "\n",
    "# VERSION_LIST = [f'2.12.1.{v}t-2.23.2.{v}i' for v in [0]]\n",
    "# # VERSION_LIST += [f'2.12.1.{v}t-2.23.1.{v}i' for v in [0,1,2]]\n",
    "# # VERSION_LIST = [f'2.12.1.{v}t-2.16.{i}.{v}i' for i in [3,4] for v in range(5)]\n",
    "\n",
    "# for VERSION in VERSION_LIST:\n",
    "#     if not HUMAN_TEST:\n",
    "#         if ASR == 'AssemblyAI':\n",
    "#             rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "#             test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "#             orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "#         else:\n",
    "#             rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "#             test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "#             orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "#     else:\n",
    "#         rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "#         test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "#         orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "#     with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "#         rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "#     with open(test_dataset_path, 'r') as f:\n",
    "#         test_dataset = json.load(f)\n",
    "#     with open(orig_dev_path, 'r') as f:\n",
    "#         orig_dev_dataset = json.load(f)\n",
    "\n",
    "#     # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "#     ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "#     pred_idx = 0\n",
    "\n",
    "#     ref_list = []\n",
    "#     hyp_list = []\n",
    "\n",
    "#     for d in tqdm(test_dataset, desc=f'VERSION {VERSION}'):\n",
    "#         if len(d) == 0:\n",
    "#             continue\n",
    "\n",
    "#         c = d[0]\n",
    "\n",
    "#         p = rewriter_ILM_preds[pred_idx]\n",
    "#         _o_idx = c['original_id']\n",
    "#         o = orig_dev_dataset[_o_idx]\n",
    "#         assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "#         assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "#         # Debug \n",
    "#         # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "#         _db_id = o['db_id']\n",
    "\n",
    "#         # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "#         # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "#         _tags = p['rewriter_tags']\n",
    "#         _rewrite_seq = p['rewrite_seq_prediction']\n",
    "#         _question_toks = c['question_toks']\n",
    "\n",
    "#         _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "#         _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "#         _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "\n",
    "#         _gold_sql = c['query']\n",
    "#         _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "#         # Save the taggerILM raw outputs, for later aggregation \n",
    "#         c['pred_tags'] = p['rewriter_tags']\n",
    "#         c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "#         c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "#         # Save prediction results \n",
    "#         c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "#         c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "#         p['gold_sql'] = _gold_sql\n",
    "#         c['score'] = p['score'] = _score\n",
    "#         c['exact'] = p['exact'] = _exact\n",
    "#         c['exec'] = p['exec'] = _exec\n",
    "\n",
    "#         _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "#         _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "#         ref_list.append([_gold_question_toks])\n",
    "#         hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "#         pred_idx += len(d)\n",
    "\n",
    "#     # Only using the 1st candidate to rewrite \n",
    "#     _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "#     _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "#     _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "#     ## Std-dev (1st cand only)\n",
    "#     _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "#     ## BLEU \n",
    "#     _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "#                         hypotheses=hyp_list)\n",
    "\n",
    "#     print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "#     print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "#     # print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "#     print('avg = {:.4f}'.format(_avg_1st))\n",
    "#     print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "#     print(f'BLEU = {_bleu:.4f}')\n",
    "#     print('='*55)\n",
    "    \n",
    "#     # Non ASR\n",
    "#     if not HUMAN_TEST:\n",
    "#         if ASR == 'AssemblyAI':\n",
    "#             test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "#         else:\n",
    "#             test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "#     else:\n",
    "#         test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "#     # ASR\n",
    "#     # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "#     # Mixed\n",
    "#     # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{VERSION}.json'\n",
    "\n",
    "#     with open(test_output_path, 'w') as f:\n",
    "#         json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1f2dc700e5462ea5f62268f0c0eae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.18.3.3i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['mp4'], ['BK'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['millisecond']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['4'], ['back', 'stage']]\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.18.3.3i ====================\n",
      "avg_exact = 0.5192\n",
      "avg = 0.7645\n",
      "avg_exec = 0.3729\n",
      "BLEU = 0.8695\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167021541e854d08b1e374b40ac9573b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.4t-2.18.3.4i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], [\"'SF\", \"'\"], ['ids'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'B-DEL', 'L-DEL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['destroyed', 'a']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "\n",
      "==================== VERSION: 2.12.1.4t-2.18.3.4i ====================\n",
      "avg_exact = 0.5302\n",
      "avg = 0.7722\n",
      "avg_exec = 0.3839\n",
      "BLEU = 0.8806\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3a9bdf5f5746248d8fdb866f0bd8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.19.3.3i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['BK'], ['id'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'B-EDIT', 'I-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['``', 'Tony', 'Award', \"''\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['fly'], ['gpa'], ['and'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['where'], ['?']]\n",
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.19.3.3i ====================\n",
      "avg_exact = 0.5393\n",
      "avg = 0.7785\n",
      "avg_exec = 0.3821\n",
      "BLEU = 0.8775\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14734734a7a24e7d89a63b7187a5497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.4t-2.19.3.4i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Select']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Select']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'B-EDIT', 'L-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['id'], ['named'], ['named', '``'], ['Database', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['substring'], ['``', 'Diana', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['authors']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['US', '?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['is'], ['id', 'is'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [[\"'Amisulpride\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['2000']]\n",
      "\n",
      "==================== VERSION: 2.12.1.4t-2.19.3.4i ====================\n",
      "avg_exact = 0.5174\n",
      "avg = 0.7615\n",
      "avg_exec = 0.3803\n",
      "BLEU = 0.8713\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b30b5217fd484e885b1660bd44c94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.2t-2.21.1.2i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'B-DEL', 'L-DEL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['``', 'Canada', \"''\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['.']]\n",
      "\n",
      "==================== VERSION: 2.12.1.2t-2.21.1.2i ====================\n",
      "avg_exact = 0.5283\n",
      "avg = 0.7723\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8712\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c97a98191c443b80d7e7e3e973029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.21.1.3i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['a', 'higher'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['What', 'are']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Debit']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], [\"'GV\", \"'\"], [\"'SF\", \"'\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'I-EDIT', 'I-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ORG']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Donor', 'Hall']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['a', 'higher']]\n",
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['the']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Return'], ['grade']]\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.21.1.3i ====================\n",
      "avg_exact = 0.5229\n",
      "avg = 0.7671\n",
      "avg_exec = 0.3729\n",
      "BLEU = 0.8664\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bf0f1e4b47480195ab9c9b1daccc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.4t-2.21.1.4i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['and'], ['named', \"'Kolob\"], [\"'GT\", \"'\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['height'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-DEL', 'B-EDIT', 'L-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['id'], ['checking', 'id'], ['named', '``'], ['Human', 'Resource', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['CV'], ['BK'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'I-EDIT', 'I-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [[\"'King\", 'Book', \"'\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'U-DEL', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['belong'], ['?']]\n",
      "\n",
      "==================== VERSION: 2.12.1.4t-2.21.1.4i ====================\n",
      "avg_exact = 0.5027\n",
      "avg = 0.7590\n",
      "avg_exec = 0.3711\n",
      "BLEU = 0.8733\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.18.3.{v}i' for v in [3,4]]\n",
    "VERSION_LIST += [f'2.12.1.{v}t-2.19.3.{v}i' for v in [3,4]]\n",
    "VERSION_LIST += [f'2.12.1.{v}t-2.21.1.{v}i' for v in [2,3,4]]\n",
    "# VERSION_LIST += [f'2.12.1.{v}t-2.20.0.{v}i' for v in [0,1,2,3,4]]\n",
    "# VERSION_LIST = [f'2.18.2.{v}i-oracle-tags' for v in range(5)]\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    # Non ASR\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "    # ASR\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "    # Mixed\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{VERSION}.json'\n",
    "\n",
    "    \n",
    "    Full_evaluate_ILM(eval_version=VERSION,\n",
    "                      rewriter_ILM_pred_path=rewriter_ILM_pred_path,\n",
    "                      test_dataset_path=test_dataset_path,\n",
    "                      orig_dev_path=orig_dev_path,\n",
    "                      test_output_path=test_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewrite_seq BLEU/WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bbdfa01f21400fa914f7cfaf6e721b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.0t-2.32.0.0i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.0t-2.32.0.0i ====================\n",
      "BLEU = 0.8740\n",
      "WER = 532 / 7146 = 0.0744\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6721836c66429e94e265f1a215507b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.1t-2.32.0.1i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.1t-2.32.0.1i ====================\n",
      "BLEU = 0.8857\n",
      "WER = 469 / 7146 = 0.0656\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72612c771f3541af92cdd41391ef0dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.2t-2.32.0.2i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.2t-2.32.0.2i ====================\n",
      "BLEU = 0.8734\n",
      "WER = 514 / 7146 = 0.0719\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a675b15f8aff42a298603f31826f5cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.32.0.3i', max=547, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.3t-2.32.0.3i ====================\n",
      "BLEU = 0.8822\n",
      "WER = 479 / 7146 = 0.0670\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "## rewrite_seq BLEU (turns out that allennlp BLEU is different from nltk BLEU)\n",
    "## also, predicted rewrite_seq is based on predicted tags; thus, use oracle rewrite instead of gold rewrite \n",
    "\n",
    "VERSION_LIST = [f'2.12.1.{v}t-2.32.0.{v}i' for v in range(4)]\n",
    "# VERSION_LIST += [f'2.12.1.{v}t-2.23.1.{v}i' for v in [0,1,2]]\n",
    "# VERSION_LIST = [f'2.12.1.{v}t-2.16.{i}.{v}i' for i in [3,4] for v in range(5)]\n",
    "\n",
    "_bleu_list = []\n",
    "_wer_list = []\n",
    "_rewrite_bleu_list = []\n",
    "_rewrite_wer_list = []\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "#     if not HUMAN_TEST:\n",
    "#         if ASR == 'AssemblyAI':\n",
    "#             rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "#             test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "#             orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "#         else:\n",
    "#             rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "#             test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "#             orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "#     else:\n",
    "#         rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "#         test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "#         orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "\n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    wer_numer = 0\n",
    "    wer_denom = 0\n",
    "    \n",
    "    ref_list_rew = []\n",
    "    hyp_list_rew = []\n",
    "    wer_numer_rew = 0\n",
    "    wer_denom_rew = 0\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}'):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']).lower() == p['question'].lower(), (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        # Debug \n",
    "        # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "        # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "        _tags = p['rewriter_tags']\n",
    "        _rewrite_seq = [t for t in p['rewrite_seq_prediction'] if t != END_SYMBOL] + [END_SYMBOL]\n",
    "        \n",
    "#         _gold_rewrite_list = []\n",
    "#         for _edit in c['rewriter_edits']:\n",
    "#             _gold_rewrite_list.extend(_edit['tgt_text'].split(' '))\n",
    "#             _gold_rewrite_list.append('[ANS]')\n",
    "#         _gold_rewrite_list.append(END_SYMBOL)\n",
    "\n",
    "#         _save = dict()\n",
    "#         Oracle_ILM_rewrite(c, p, rewrite_seq_toks_save=_save)\n",
    "#         _gold_rewrite_list = _save['rewrite_seq_toks']\n",
    "#         _gold_rewrite_list.append(END_SYMBOL)\n",
    "        \n",
    "#         _gold_rewrite_list = [_t.lower() for _t in _gold_rewrite_list]\n",
    "        _rewrite_seq = [_t.lower() for _t in _rewrite_seq]\n",
    "        \n",
    "        ref_list_rew.append([_gold_rewrite_list])\n",
    "        hyp_list_rew.append(_rewrite_seq)\n",
    "        wer_numer_rew += editdistance.eval(_gold_rewrite_list, _rewrite_seq)\n",
    "        wer_denom_rew += len(_gold_rewrite_list)\n",
    "        \n",
    "#         print(p['question'])\n",
    "#         print(_rewrite_seq)\n",
    "#         print(_gold_rewrite_list)\n",
    "\n",
    "#         _rewritten_question_toks = _Postprocess_rewrite_seq_wrapper(c, p)\n",
    "        _rewritten_question_toks = p['rewrite_seq_prediction']\n",
    "        _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "        \n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "        wer_numer += editdistance.eval(_gold_question_toks, _rewritten_question_toks)\n",
    "        wer_denom += len(_gold_question_toks)\n",
    "        \n",
    "        pred_idx += len(d)\n",
    "        \n",
    "    ## BLEU \n",
    "#     _rewrite_bleu = corpus_bleu(list_of_references=ref_list_rew,\n",
    "#                         hypotheses=hyp_list_rew)\n",
    "#     _rewrite_bleu_list.append(_rewrite_bleu)\n",
    "    \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    _bleu_list.append(_bleu)\n",
    "    \n",
    "    _wer = wer_numer / wer_denom\n",
    "    _wer_list.append(_wer)\n",
    "    \n",
    "#     _wer_rew = wer_numer_rew / wer_denom_rew\n",
    "#     _rewrite_wer_list.append(_wer_rew)\n",
    "    \n",
    "    print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print(f'WER = {wer_numer} / {wer_denom} = {_wer:.4f}')\n",
    "#     print(f'Rewrite_seq BLEU = {_rewrite_bleu:.4f}')\n",
    "#     print(f'Rewrite_seq WER = {wer_numer_rew} / {wer_denom_rew} = {_wer_rew:.4f}')\n",
    "    print('='*55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\t0.0744\n",
      "0.8857\t0.0656\n",
      "0.8734\t0.0719\n",
      "0.8822\t0.0670\n"
     ]
    }
   ],
   "source": [
    "# print('\\n'.join([f'{p[0]:.4f}\\t{p[1]:.4f}\\t{p[2]:.4f}\\t{p[3]:.4f}' for p in zip(_bleu_list, _wer_list, _rewrite_bleu_list, _rewrite_wer_list)]))\n",
    "print('\\n'.join([f'{p[0]:.4f}\\t{p[1]:.4f}' for p in zip(_bleu_list, _wer_list)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.pyenv/versions/3.7.5/envs/py3.7-speakql/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2213386697554703e-77"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([[[1,2,3]]], [[1,2,3]], smoothing_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze/Modify POS\n",
    "- TODO: use func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Postprocess_POS_wrapper:\n",
    "    def __init__(self, POS, mode):\n",
    "        self.POS = POS\n",
    "        assert mode in ('freeze', 'modify'), mode\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __call__(self, cand_dict, pred_dict):\n",
    "        _tags = pred_dict['rewriter_tags']\n",
    "        _rewrite_seq = pred_dict['rewrite_seq_prediction']\n",
    "        _question_toks = cand_dict['question_toks']\n",
    "        \n",
    "        if self.mode == 'freeze':\n",
    "            _rewritten_toks = Postprocess_rewrite_seq_freeze_POS(\n",
    "                _tags,\n",
    "                _rewrite_seq,\n",
    "                _question_toks,\n",
    "                freeze_POS=self.POS,\n",
    "                nlp=nlp)\n",
    "        else:\n",
    "            _rewritten_toks = Postprocess_rewrite_seq_modify_POS(\n",
    "                _tags,\n",
    "                _rewrite_seq,\n",
    "                _question_toks,\n",
    "                modify_POS=self.POS,\n",
    "                nlp=nlp)\n",
    "        return _rewritten_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49227d06ebbc44ffa1091d7c0461d7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-PUNCT-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: CoreNLP connection timeout. Recreating the server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-PUNCT-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5082\n",
      "avg = 0.7587\n",
      "avg_exec = 0.3766\n",
      "BLEU = 0.8451\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13541f4f7e643d5852a95c5375712a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-NUM-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-NUM-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5137\n",
      "avg = 0.7629\n",
      "avg_exec = 0.3766\n",
      "BLEU = 0.8680\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd3d98a22064e0280ed08b8ddfbe9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-VERB-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-VERB-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5247\n",
      "avg = 0.7712\n",
      "avg_exec = 0.3748\n",
      "BLEU = 0.8679\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561a265aa2a040b49f345b360c440d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-PRON-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-PRON-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5155\n",
      "avg = 0.7654\n",
      "avg_exec = 0.3711\n",
      "BLEU = 0.8654\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9a0641a5df4bd9873be6cfee81000b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-ADP-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-ADP-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5174\n",
      "avg = 0.7621\n",
      "avg_exec = 0.3729\n",
      "BLEU = 0.8657\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26be6294869c4142a7f083a292c6bc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-NOUN-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-NOUN-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5101\n",
      "avg = 0.7612\n",
      "avg_exec = 0.3583\n",
      "BLEU = 0.8543\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632c8b17180b4510abd8c1b275989ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-AUX-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-AUX-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5283\n",
      "avg = 0.7722\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8712\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bfa4fdf9314968b5f56765851325ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-DET-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-DET-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5247\n",
      "avg = 0.7671\n",
      "avg_exec = 0.3711\n",
      "BLEU = 0.8687\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8d750700a24d0aa5688ebfb8216b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-SCONJ-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-SCONJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5265\n",
      "avg = 0.7714\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8729\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a448cb7b74a34f79a58a128de1af0488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-PART-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-PART-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5265\n",
      "avg = 0.7714\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8729\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f674eed2b7d0421981f6c0be0b76ad29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-ADJ-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-ADJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5265\n",
      "avg = 0.7696\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8725\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbc0e3002df407897445fe9b2637695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-CCONJ-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-CCONJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5265\n",
      "avg = 0.7711\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8695\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0044662e9e7d4defa4273392ff1cc489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-ADV-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Freeze-ADV-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5302\n",
      "avg = 0.7744\n",
      "avg_exec = 0.3821\n",
      "BLEU = 0.8730\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f471278fa99f474b8e56197ec7137a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Freeze-PROPN-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Freeze-PROPN-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.5174\n",
      "avg = 0.7664\n",
      "avg_exec = 0.3766\n",
      "BLEU = 0.8740\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.18.2.{v}i' for v in [2]]\n",
    "\n",
    "POS_LIST = [\"PUNCT\", \"NUM\", \"VERB\", \"PRON\", \"ADP\", \"NOUN\", \"AUX\", \"DET\",\n",
    "            \"SCONJ\", \"PART\", \"ADJ\", \"CCONJ\", \"ADV\", \"PROPN\"]\n",
    "\n",
    "GROUP_LIST = [(_v, _p) for _v in VERSION_LIST for _p in POS_LIST]\n",
    "\n",
    "for VERSION, POS in GROUP_LIST:\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}-freeze={POS}.json'\n",
    "    \n",
    "    _rewriter = _Postprocess_POS_wrapper(POS, 'freeze')\n",
    "    \n",
    "    Full_evaluate_ILM(eval_version=f\"Freeze-{POS}-{VERSION}\",\n",
    "                      rewriter_ILM_pred_path=rewriter_ILM_pred_path,\n",
    "                      test_dataset_path=test_dataset_path,\n",
    "                      orig_dev_path=orig_dev_path,\n",
    "                      test_output_path=test_output_path,\n",
    "                      ILM_rewrite_func=_rewriter)\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d8d42f56c646c99867ed556fc7510d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-PUNCT-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-PUNCT-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4753\n",
      "avg = 0.7387\n",
      "avg_exec = 0.3419\n",
      "BLEU = 0.8257\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eaff613e044bd485988a55b75cb71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-NUM-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-NUM-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4698\n",
      "avg = 0.7353\n",
      "avg_exec = 0.3419\n",
      "BLEU = 0.8055\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede75d2252f74b1bb8c12b2814a7fc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-VERB-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-VERB-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4607\n",
      "avg = 0.7292\n",
      "avg_exec = 0.3437\n",
      "BLEU = 0.8057\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac923ab13b64b3e8422691d0874fad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-PRON-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-PRON-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4698\n",
      "avg = 0.7347\n",
      "avg_exec = 0.3492\n",
      "BLEU = 0.8058\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8e543a8c2c4a3580bb8c024ac84417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-ADP-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-ADP-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4662\n",
      "avg = 0.7365\n",
      "avg_exec = 0.3455\n",
      "BLEU = 0.8077\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fb8a03b4c0420b8c74275f8f77f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-NOUN-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-NOUN-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4698\n",
      "avg = 0.7307\n",
      "avg_exec = 0.3583\n",
      "BLEU = 0.8182\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58136f6d8f6844e984f1909007b26161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-AUX-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-AUX-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4534\n",
      "avg = 0.7262\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8022\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea07a6abe18a4db1b2daa9b6ed203123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-DET-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-DET-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4625\n",
      "avg = 0.7321\n",
      "avg_exec = 0.3492\n",
      "BLEU = 0.8044\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee55b61f16c74bf698bb1d9c05bae399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-SCONJ-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-SCONJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7273\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8010\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fd05fe4dd34b16b84df13ef10f2846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-PART-2.12.1.2t-2.18.2.2i', max=547, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-PART-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7273\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8010\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323816b02b1a4e49bfbdb16f67a5233f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-ADJ-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-ADJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7269\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8013\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df3ca4847da4a74917a77f1af28ccd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-CCONJ-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-CCONJ-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7273\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8042\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51fcd86fbc14d8b86590e5835a2d257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-ADV-2.12.1.2t-2.18.2.2i', max=547, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: Modify-ADV-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4534\n",
      "avg = 0.7244\n",
      "avg_exec = 0.3364\n",
      "BLEU = 0.8009\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc532eaaf7184dd582d8333c18e6f572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION Modify-PROPN-2.12.1.2t-2.18.2.2i', max=547, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: Modify-PROPN-2.12.1.2t-2.18.2.2i ====================\n",
      "avg_exact = 0.4735\n",
      "avg = 0.7318\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8004\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.18.2.{v}i' for v in [2]]\n",
    "\n",
    "POS_LIST = [\"PUNCT\", \"NUM\", \"VERB\", \"PRON\", \"ADP\", \"NOUN\", \"AUX\", \"DET\",\n",
    "            \"SCONJ\", \"PART\", \"ADJ\", \"CCONJ\", \"ADV\", \"PROPN\"]\n",
    "\n",
    "GROUP_LIST = [(_v, _p) for _v in VERSION_LIST for _p in POS_LIST]\n",
    "\n",
    "for VERSION, POS in GROUP_LIST:\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}-modify={POS}.json'\n",
    "    \n",
    "    _rewriter = _Postprocess_POS_wrapper(POS, 'modify')\n",
    "    \n",
    "    Full_evaluate_ILM(eval_version=f\"Modify-{POS}-{VERSION}\",\n",
    "                      rewriter_ILM_pred_path=rewriter_ILM_pred_path,\n",
    "                      test_dataset_path=test_dataset_path,\n",
    "                      orig_dev_path=orig_dev_path,\n",
    "                      test_output_path=test_output_path,\n",
    "                      ILM_rewrite_func=_rewriter)\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained tagger + Oracle ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oracle_ILM_rewrite(cand_dict, pred_dict, rewrite_seq_toks_save=None):\n",
    "    ## rewrite_seq_toks_save: Dict, used to save the computed rewrite_seq_toks \n",
    "    \n",
    "    tags = pred_dict['rewriter_tags']\n",
    "    question_toks = cand_dict['question_toks']\n",
    "    \n",
    "    gold_toks = cand_dict['gold_question_toks']\n",
    "    alignment_span_pairs = cand_dict['alignment_span_pairs']  # List[Tuple[List: src_span, List: tgt_span]]\n",
    "    question_toks_rewritten = []\n",
    "    rewrite_seq_toks = []\n",
    "\n",
    "    _last_src_idx = -1\n",
    "    for src_span, tgt_span in alignment_span_pairs:\n",
    "        for i in range(_last_src_idx + 1, src_span[0]):\n",
    "            # ignored src tokens \n",
    "            if tags[i].endswith('KEEP'):\n",
    "                question_toks_rewritten.append(question_toks[i])\n",
    "            else:\n",
    "                # del or edit \n",
    "                pass\n",
    "        \n",
    "        if len(src_span) == len(tgt_span) or len(src_span) == 1:\n",
    "            # treat src as single tokens \n",
    "            if len(src_span) == len(tgt_span):\n",
    "                _src_spans = [[_idx] for _idx in src_span]\n",
    "                _tgt_spans = [[_idx] for _idx in tgt_span]\n",
    "            else:\n",
    "                _src_spans = [src_span]\n",
    "                _tgt_spans = [tgt_span]\n",
    "                \n",
    "            for _src_span, _tgt_span in zip(_src_spans, _tgt_spans):\n",
    "                _idx = _src_span[0]\n",
    "                if tags[_idx].endswith('KEEP'):\n",
    "                    question_toks_rewritten.append(question_toks[_idx])\n",
    "                elif tags[_idx].endswith('DEL'):\n",
    "                    pass\n",
    "                elif tags[_idx].endswith('EDIT'):\n",
    "                    # edit to correct, i.e. append corresponding gold tokens \n",
    "                    for j in _tgt_span:\n",
    "                        question_toks_rewritten.append(gold_toks[j])\n",
    "                        rewrite_seq_toks.append(gold_toks[j])\n",
    "                        if tags[_idx] in {'U-EDIT', 'L-EDIT'}:\n",
    "                            rewrite_seq_toks.append('[ANS]')\n",
    "        else:\n",
    "            # multi-token            \n",
    "            if all([tags[i].endswith('DEL') for i in src_span]):\n",
    "                # if all del, del \n",
    "                pass\n",
    "            elif all([tags[i].endswith('DEL') or tags[i].endswith('EDIT') for i in src_span]):\n",
    "                # if all edit or del (at least 1 edit), edit \n",
    "                for j in tgt_span:\n",
    "                    question_toks_rewritten.append(gold_toks[j])\n",
    "            else:\n",
    "                # otherwise (if has keep), keep; if not all keep, print warning \n",
    "                assert any([tags[i].endswith('KEEP') for i in src_span])\n",
    "                if not all([tags[i].endswith('KEEP') for i in src_span]):\n",
    "                    print(f'Unaddressed mismatch (treated as KEEP): {pred_dict[\"original_id\"]}')\n",
    "                    print('Target', [gold_toks[j] for j in tgt_span])\n",
    "                    print('Source', [(question_toks[i], tags[i]) for i in src_span])\n",
    "\n",
    "        _last_src_idx = src_span[-1]\n",
    "                    \n",
    "    if rewrite_seq_toks_save is not None:\n",
    "        rewrite_seq_toks_save['rewrite_seq_toks'] = rewrite_seq_toks\n",
    "            \n",
    "    return question_toks_rewritten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in test_dataset:\n",
    "    c = s[0]\n",
    "    if c['original_id'] == 193:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_ILM_preds:\n",
    "    if p['original_id'] == 193:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaddressed mismatch: 193\n",
      "['abbreviation', \"'UAL\", \"'\"]\n",
      "[('abbreviation', 'O-KEEP'), (\"you'il\", 'U-EDIT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['which', 'airline', 'has', '?']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Oracle_ILM_rewrite(c, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.18.2.{v}i' for v in [0,1,2,3,4]]\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    _tagger_ver = VERSION.split('-')[0]\n",
    "    \n",
    "    # Non ASR\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{_tagger_ver}-oracle-ILM.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{_tagger_ver}-oracle-ILM.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{_tagger_ver}-oracle-ILM.json'\n",
    "    # ASR\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{_tagger_ver}-oracle-ILM.json'\n",
    "    # Mixed\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{_tagger_ver}-oracle-ILM.json'\n",
    "\n",
    "        \n",
    "    Full_evaluate_ILM(eval_version=VERSION,\n",
    "                      rewriter_ILM_pred_path=rewriter_ILM_pred_path,\n",
    "                      test_dataset_path=test_dataset_path,\n",
    "                      orig_dev_path=orig_dev_path,\n",
    "                      test_output_path=test_output_path,\n",
    "                      ILM_rewrite_func=Oracle_ILM_rewrite)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output to test_rewriter (no eval)\n",
    "- Use py script, based on ILM-output-to-dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from predicted file (only 1st cand is predicted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.6.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using EvaluateSQL_full \n",
    "\n",
    "# tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "# kmaps = evaluation.build_foreign_key_map_from_json(tables_json)\n",
    "\n",
    "plist = [d[0]['pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reranker output file \n",
    "VERSION = '1.10.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [c['ratsql_pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection'] == 1]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [d[0]['ratsql_pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using all ASR candidates (no longer in use)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']\n",
    "        _tags = p['tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation process with oracle tags (no longer in use for version>=2.3.0)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _tags = p['gold_tags']\n",
    "        _rewrite_seq = p['oracle_tags_rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question'] = _rewritten_question\n",
    "        c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql'] = _pred_sql\n",
    "        c['oracle_tags_score'] = p['oracle_tags_score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['oracle_tags_score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['oracle_tags_score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_oracle_avg_1st = sum([d[0]['oracle_tags_score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_oracle_std_1st = np.std([d[0]['oracle_tags_score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_oracle_avg_1st, _oracle_std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['tag_prediction'] = list(zip(p['question'].split(' '), p['tags_prediction']))\n",
    "        _pred_c['rewrite_seq'] = []\n",
    "        for t in p['rewrite_seq_prediction']:\n",
    "            _pred_c['rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['rewritten_question'] = p['rewritten_question']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_tags'] = list(zip(p['question'].split(' '), p['gold_tags']))\n",
    "        _pred_c['oracle_tags_rewrite_seq'] = []\n",
    "        for t in p['oracle_tags_rewrite_seq_prediction']:\n",
    "            _pred_c['oracle_tags_rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question']\n",
    "        _pred_c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql']\n",
    "        _pred_c['oracle_tags_score'] = p['oracle_tags_score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test-prediction-{}.json'.format(VERSION), 'w') as f:\n",
    "    json.dump(test_pred_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset file with predictions \n",
    "\n",
    "with open('./output/pred-{}.json'.format(VERSION), 'r') as f:\n",
    "    test_pred_dataset = json.load(f)\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis \n",
    "orig_dev_preds_path = './output/dev_output.txt'\n",
    "\n",
    "with open(orig_dev_preds_path, 'r') as f:\n",
    "    orig_dev_preds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "len(orig_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for d in test_dataset[5::40]:\n",
    "#     print('DB:', d[0]['db_id'])\n",
    "#     print('ASR question:\\t\\t', d[0]['question'])\n",
    "#     print('Rewritten question:\\t', d[0]['rewritten_question'])\n",
    "#     print('Gold question:\\t\\t', d[0]['gold_question'])\n",
    "#     print('ASR-q Pred SQL:\\t\\t', d[0]['ratsql_pred_sql'])\n",
    "#     print('Rewritten-q Pred SQL:\\t', d[0]['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', orig_dev_preds[d[0]['original_id']])\n",
    "#     print('Gold SQL:\\t\\t', d[0]['query'])\n",
    "#     print('Score:', d[0]['score'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_samples = [c for d in test_pred_dataset for c in d]\n",
    "for i, c in list(enumerate(test_pred_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['ASR_question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "    print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['gold_sql'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = [8, 96, 272, 448, 1416, 1592, 1680, 1856, 2120, 2296, 2384, 2560, 2824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (Amazon): VERSION = 3.12.1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbadc38d54ef4598b38e1fb716ad31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSION 3.12.1.1:\n",
      "avg = 0.7410 (std = 0.3078)\n",
      "avg_exact = 0.4826\n",
      "avg_exec = 0.3638\n",
      "BLEU = 0.8532\n",
      "\n",
      "Evaluating (Amazon): VERSION = 3.12.1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb02e39911f48f08c1f045f67cbcb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.12.1.2:\n",
      "avg = 0.7478 (std = 0.3037)\n",
      "avg_exact = 0.4899\n",
      "avg_exec = 0.3675\n",
      "BLEU = 0.8566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Batch evaluating \n",
    "\n",
    "VERSION_LIST = ['3.12.1.1', '3.12.1.2']\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    print(f'Evaluating ({ASR}){\" (human test)\" if HUMAN_TEST else \"\"}: VERSION = {VERSION}')\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        # human test \n",
    "        rewriter_s2s_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    \n",
    "    with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "        rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "        \n",
    "    # Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    \n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags']\n",
    "        # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        # _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "\n",
    "        if _rewritten_question == '':\n",
    "            print(f'_rewritten_question is empty')\n",
    "            _pred_sql = ''\n",
    "            _gold_sql = c['query']\n",
    "            _exact = _score = _exec = 0\n",
    "        else:\n",
    "            _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "            _gold_sql = c['query']\n",
    "            _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "        \n",
    "        # For BLEU \n",
    "        _rewritten_question_toks = [_t.lower() for _t in p['s2s_prediction']]\n",
    "        _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "    \n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    \n",
    "    print(f'VERSION {VERSION}:')\n",
    "    print(f'avg = {_avg_1st:.4f} (std = {_std_1st:.4f})')\n",
    "    print(f'avg_exact = {_avg_exact_1st:.4f}')\n",
    "    print(f'avg_exec = {_avg_exec_1st:.4f}')\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print()\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "        \n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single version, not used \n",
    "\n",
    "VERSION = '3.3.0.0'\n",
    "\n",
    "rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "    rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_preds[3::300]:\n",
    "    print(p['question'])\n",
    "    print(' '.join(p['s2s_prediction']))\n",
    "    print(' '.join(p['gold_rewrite_seq_s2s'][1:-1]))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags']\n",
    "    # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    # _question_toks = c['question_toks']\n",
    "\n",
    "    # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "    \n",
    "    if _rewritten_question == '':\n",
    "        print(f'_rewritten_question is empty')\n",
    "        _pred_sql = ''\n",
    "        _gold_sql = c['query']\n",
    "        _score = 0\n",
    "    else:\n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "\n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actual (full) evaluation process \n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        pred_idx += 1\n",
    "        if 'score' in c:\n",
    "            continue  # already inferred  \n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "        _pred_result = Question(_rewritten_question, _db_id)\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        \n",
    "        if len(_pred_result) == 0:\n",
    "            print(_db_id, _rewritten_question, '-- no predictiction')\n",
    "            _pred_sql = ''\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = _pred_result[0]['inferred_code']\n",
    "            _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_preds]) / len(rewriter_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['rewritten_question'] = p['s2s_prediction']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [c for d in test_dataset for c in d]\n",
    "for i, c in list(enumerate(test_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['query'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating predicted SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5840"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-asr_mixed/predictions_eval_23.95177918190888.json'\n",
    "\n",
    "with open(pred_dataset_path, 'r') as f:\n",
    "    pred_dataset = json.load(f)\n",
    "len(pred_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'select name, country from singer order by age desc',\n",
       " 'query': 'SELECT name ,  country ,  age FROM singer ORDER BY age DESC',\n",
       " 'question': 'Shou name country age for all singers ordered by age from the oldest to the youngest.',\n",
       " 'db_id': 'concert_singer',\n",
       " 'db_path': '/vault/uskg/data/downloads/extracted/a7a5df4a47a9fc5a13f1b3c5c9ab67d879571a0f9286430e4c3377863a1fb7a1/spider-asr-mixed/database',\n",
       " 'db_table_names': ['stadium', 'singer', 'concert', 'singer_in_concert'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   3],\n",
       "  'column_name': ['*',\n",
       "   'Stadium_ID',\n",
       "   'Location',\n",
       "   'Name',\n",
       "   'Capacity',\n",
       "   'Highest',\n",
       "   'Lowest',\n",
       "   'Average',\n",
       "   'Singer_ID',\n",
       "   'Name',\n",
       "   'Country',\n",
       "   'Song_Name',\n",
       "   'Song_release_year',\n",
       "   'Age',\n",
       "   'Is_male',\n",
       "   'concert_ID',\n",
       "   'concert_Name',\n",
       "   'Theme',\n",
       "   'Stadium_ID',\n",
       "   'Year',\n",
       "   'concert_ID',\n",
       "   'Singer_ID']},\n",
       " 'db_column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'others',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'db_primary_keys': {'column_id': [1, 8, 15, 20]},\n",
       " 'db_foreign_keys': {'column_id': [18, 21, 20], 'other_column_id': [1, 8, 15]},\n",
       " 'serialized_schema': ' | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'struct_in': '| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'text_in': 'Shou name country age for all singers ordered by age from the oldest to the youngest.',\n",
       " 'seq_out': 'select name, country, age from singer order by age desc',\n",
       " 'description': 'task: spider',\n",
       " 'section': 'test',\n",
       " 'arg_path': 'META_TUNING/A-spider_with_cell-asr_mixed.cfg'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset_first_cands = [pred_dataset[0]]\n",
    "\n",
    "for i in range(1, len(pred_dataset)):\n",
    "    if pred_dataset[i]['query'] == pred_dataset[i-1]['query']:\n",
    "        continue\n",
    "    pred_dataset_first_cands.append(pred_dataset[i])\n",
    "\n",
    "len(pred_dataset_first_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriter-adaptation Training Data\n",
    "- Can directly use the existing pipeline, just need to use the tagger-predicted dataset file and ILM output based on that, merge (put in the rewritten text) and get the final dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _detokenize(toks):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    return detokenizer.detokenize(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Tagger_and_ILM_pred(eval_version,\n",
    "                      rewriter_ILM_pred_path,\n",
    "                      test_dataset_path,   # like xxx_rewriter+phonemes.json; can be train/dev \n",
    "                      # orig_dev_path,\n",
    "                      test_output_path=None,\n",
    "                      output_flatten=True, # If true, output a flattened list of samples \n",
    "                      ILM_rewrite_func=_Postprocess_rewrite_seq_wrapper,\n",
    "                      detokenize_func=_detokenize):\n",
    "    \n",
    "    '''\n",
    "    Very similar to Full_evaluate_ILM, process each cand, but not running through NLIDB for SQL prediction \n",
    "    \n",
    "    eval_version: simply for printing results \n",
    "    \n",
    "    Example paths:\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    \n",
    "    ILM_rewrite_func: Callable, args: (_tags, _rewrite_seq, _question_toks)\n",
    "    detokenize_func: Callable, args: (toks)\n",
    "    '''\n",
    "    \n",
    "    VERSION = eval_version\n",
    "    \n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "#     with open(orig_dev_path, 'r') as f:\n",
    "#         orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}'):\n",
    "        for c in d:\n",
    "            p = rewriter_ILM_preds[pred_idx]\n",
    "            _o_idx = c['original_id']\n",
    "            # o = orig_dev_dataset[_o_idx]\n",
    "            assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "            # assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "            # Debug \n",
    "            # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "            _tags = p['rewriter_tags']\n",
    "            _rewrite_seq = p['rewrite_seq_prediction']\n",
    "            _question_toks = c['question_toks']\n",
    "\n",
    "            # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "            _rewritten_question_toks = ILM_rewrite_func(c, p)\n",
    "            # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "            _rewritten_question = detokenize_func(_rewritten_question_toks)\n",
    "\n",
    "            # Save the taggerILM raw outputs, for later aggregation \n",
    "            c['pred_tags'] = p['rewriter_tags']\n",
    "            c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "            c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "            # Save prediction results \n",
    "            # keep the asr question (just in case)\n",
    "            c['asr_question'] = c['question']\n",
    "            c['asr_question_toks'] = c['question_toks']\n",
    "            # Directly write to key \"question\" and \"question_toks\" to help later training \n",
    "            # c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "            c['question'] = _rewritten_question\n",
    "            c['question_toks'] = _rewritten_question_toks\n",
    "\n",
    "            pred_idx += 1\n",
    "\n",
    "    if test_output_path is not None:\n",
    "        if output_flatten:\n",
    "            output_struct = [c for d in test_dataset for c in d]\n",
    "        else:\n",
    "            output_struct = test_dataset\n",
    "        with open(test_output_path, 'w') as f:\n",
    "            json.dump(output_struct, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'dev-2.12.1.0t-2.29.0.0i'\n",
    "rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/dev_rewriter.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/aside/retrain-rewritten/{VERSION}.json'\n",
    "\n",
    "Merge_Tagger_and_ILM_pred(eval_version=VERSION,\n",
    "                  rewriter_ILM_pred_path=rewriter_ILM_pred_path,\n",
    "                  test_dataset_path=test_dataset_path,\n",
    "                  test_output_path=test_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format eval result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratsql\n",
      "0.5229\t0.7673\t0.3821\t0.8594\t0.0754\n",
      "0.5247\t0.7713\t0.3803\t0.8807\t0.0633\n",
      "0.5192\t0.7622\t0.3638\t0.8727\t0.0680\n",
      "0.5411\t0.7819\t0.3821\t0.8771\t0.0690\n",
      "\n",
      "uskg\n",
      "0.4863\t0.6289\t0.4845\t0.8594\t0.0754\n",
      "0.4954\t0.6270\t0.4954\t0.8807\t0.0633\n",
      "0.4826\t0.6244\t0.4698\t0.8727\t0.0680\n",
      "0.5064\t0.6377\t0.5009\t0.8771\t0.0690\n",
      "\n",
      "uskg-large\n",
      "0.5558\t0.7229\t0.5539\t0.8594\t0.0754\n",
      "0.5960\t0.7455\t0.5850\t0.8807\t0.0633\n",
      "0.5777\t0.7253\t0.5576\t0.8727\t0.0680\n",
      "0.5850\t0.7398\t0.5759\t0.8771\t0.0690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FNAME_LIST = [f'eval-humantest-yshao-1.15.2.{v}' for v in range(4)]\n",
    "# FNAME_LIST = [f'eval-2.12.1.{v}t-2.33.9.{v}i' for v in range(4)]\n",
    "# FNAME_LIST = [f'eval-dev-2.12.1.{v}t-2.33.8.{v}i' for v in range(4)]\n",
    "FNAME_LIST = [f'eval-2.12.1.{v}t-2.35.1.{v}i' for v in range(4)]\n",
    "# backend = 'uskg'\n",
    "backends = ['ratsql', 'uskg', 'uskg-large']\n",
    "\n",
    "for backend in backends:\n",
    "    print(backend)\n",
    "    for FNAME in FNAME_LIST:\n",
    "        res_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/{backend}-test-save/{FNAME}.json'\n",
    "        with open(res_path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "    #     print(f\"{d['avg_exact']:.4f}\\t{d['avg']:.4f}\\t{d['avg_exec']:.4f}\\t{d['BLEU']:.4f}\")\n",
    "        print(f\"{d['avg_exact']:.4f}\\t{d['avg']:.4f}\\t{d['avg_exec']:.4f}\\t{d['BLEU']:.4f}\\t{d['WER']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT\t0.5210\n",
      "NUM\t0.5320\n",
      "VERB\t0.5375\n",
      "PRON\t0.5283\n",
      "ADP\t0.5283\n",
      "NOUN\t0.5046\n",
      "AUX\t0.5393\n",
      "DET\t0.5375\n",
      "SCONJ\t0.5411\n",
      "PART\t0.5411\n",
      "ADJ\t0.5430\n",
      "CCONJ\t0.5411\n",
      "ADV\t0.5411\n",
      "PROPN\t0.5338\n"
     ]
    }
   ],
   "source": [
    "POS_LIST = [\"PUNCT\", \"NUM\", \"VERB\", \"PRON\", \"ADP\", \"NOUN\", \"AUX\", \"DET\",\n",
    "    \"SCONJ\", \"PART\", \"ADJ\", \"CCONJ\", \"ADV\", \"PROPN\"]\n",
    "\n",
    "# FNAME_LIST = [f'eval-1.15.1.{v}' for v in range(4)]\n",
    "# FNAME_LIST = [f'eval-2.12.1.1t-2.31.0.1i-freeze={pos}' for pos in POS_LIST]\n",
    "# FNAME_LIST = [f'eval-humantest-yshao-1.16.0.{v}' for v in range(4)]\n",
    "# FNAME_LIST = [f'eval-humantest-yshao-2.31.0.{v}i-oracle-tags' for v in range(4)]\n",
    "backend = 'ratsql'\n",
    "\n",
    "for _pos in POS_LIST:\n",
    "    _run_name = f'eval-2.12.1.1t-2.33.5.1i-freeze={_pos}'\n",
    "    res_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/{backend}-test-save/{_run_name}.json'\n",
    "    with open(res_path, 'r') as f:\n",
    "        d = json.load(f)\n",
    "    print(f\"{_pos}\\t{d['avg_exact']:.4f}\")\n",
    "#     print(f\"{d['avg_exact']:.4f}\\t{d['avg']:.4f}\\t{d['avg_exec']:.4f}\\t{d['BLEU']:.4f}\\t{d['WER']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _duration_to_hours(dur_str):\n",
    "    if 'day' not in dur_str:\n",
    "        _nday = 0\n",
    "        _hour_str = dur_str\n",
    "    else:\n",
    "        _day_str, _hour_str = dur_str.split(' day, ')\n",
    "        _nday = int(_day_str)\n",
    "        \n",
    "    _time = datetime.datetime.strptime(_hour_str.split('.')[0], \"%H:%M:%S\")\n",
    "    _hours = _nday * 24 + _time.hour + _time.minute / 60 + _time.second / 3600\n",
    "    \n",
    "    return _hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.746944444444445"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_duration_to_hours(\"19:44:49.731672\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35.1.0i\n",
      "39 -> 39\n",
      "2.35.1.1i\n",
      "29 -> 29\n",
      "2.35.1.2i\n",
      "26 -> 26\n",
      "2.35.1.3i\n",
      "30 -> 30\n",
      "6.2694\t4.6453\t4.2556\t4.8972\n"
     ]
    }
   ],
   "source": [
    "# VERSION_LIST = [f'2.12.1.{v}t' for v in range(4)]\n",
    "VERSION_LIST = [f'2.35.1.{v}i' for v in range(4)]\n",
    "# VERSION_LIST = [f'1.15.2.{v}' for v in range(4)]\n",
    "\n",
    "_hours_list = []\n",
    "_patience = 20\n",
    "\n",
    "for ver in VERSION_LIST:\n",
    "    print(ver)\n",
    "    res_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/runs/{ver}/metrics.json'\n",
    "    with open(res_path, 'r') as f:\n",
    "        d = json.load(f)\n",
    "    \n",
    "    _dur_str = d['training_duration']\n",
    "    _hours = _duration_to_hours(_dur_str)\n",
    "    \n",
    "    ## Unify the patience \n",
    "    _epochs = d['training_epochs']\n",
    "    _best_ep = d['best_epoch']\n",
    "    print(_epochs, '->', _best_ep + _patience - 1)\n",
    "    \n",
    "    _hours = _hours / _epochs * (_best_ep + _patience - 1)\n",
    "    \n",
    "    _hours_list.append(_hours)\n",
    "\n",
    "print('\\t'.join([f\"{_h:.4f}\" for _h in _hours_list]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_ILM_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dev_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite_seq postprocessing, to get the rewritten question \n",
    "\n",
    "_idx = 154\n",
    "\n",
    "_tags = rewriter_ILM_preds[_idx]['tags_prediction']\n",
    "_rewrite_seq = rewriter_ILM_preds[_idx]['rewrite_seq_prediction']\n",
    "_question_toks = rewriter_ILM_preds[_idx]['question'].split(' ')\n",
    "_tags, _rewrite_seq, _question_toks\n",
    "\n",
    "postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'concert_singer'\n",
    "g_str = 'SELECT count(*) FROM singer'\n",
    "p_str = \"SELECT Count(DISTINCT singer.Name) FROM singer WHERE singer.Name = 'terminal'\"\n",
    "\n",
    "db, p_str, g_str, EvaluateSQL(p_str, g_str, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 41112)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/train/train_reranker.json') as f:\n",
    "    _train = json.load(f)\n",
    "len(_train), sum([len(d) for d in _train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 623.44 MiB, increment: -2.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('While', 'SCONJ'),\n",
       " ('the', 'DET'),\n",
       " ('i', 'PRON'),\n",
       " ('d', 'NOUN'),\n",
       " ('between', 'ADP'),\n",
       " ('kyle', 'PROPN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('jetblue', 'PROPN'),\n",
       " ('airways', 'NOUN'),\n",
       " ('are', 'AUX'),\n",
       " ('good', 'ADJ')]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_d = nlp(\"While the id between kyle and jetblue airways are good\")\n",
    "list(zip([t.text for t in _d], [t.pos_ for t in _d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[540223612631,\n",
       " 178720741399,\n",
       " 109777393677,\n",
       " 817735554319,\n",
       " 340641764082,\n",
       " 582783668677,\n",
       " 739916569103,\n",
       " 548295624987,\n",
       " 793475247955,\n",
       " 140832154574]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.randint(1, 1e12) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
