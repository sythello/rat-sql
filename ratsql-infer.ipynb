{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "from sys import modules\n",
    "import _jsonnet\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/third_party/wikisql'))\n",
    "# sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.datasets.spider import SpiderItem\n",
    "from ratsql.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation\n",
    "from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modules['SpeakQL.Allennlp_models.utils.misc_utils']\n",
    "# del EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq\n",
    "# from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Rat_sql(root_dir,\n",
    "                 exp_config_path,\n",
    "                 model_dir,\n",
    "                 checkpoint_step=40000):\n",
    "\n",
    "    exp_config = json.loads(_jsonnet.evaluate_file(exp_config_path))\n",
    "    \n",
    "    model_config_path = os.path.join(root_dir, exp_config[\"model_config\"])\n",
    "    model_config_args = exp_config.get(\"model_config_args\")\n",
    "    \n",
    "    infer_config = json.loads(_jsonnet.evaluate_file(model_config_path, tla_codes={'args': json.dumps(model_config_args)}))\n",
    "\n",
    "    inferer = Inferer(infer_config)\n",
    "    inferer.device = torch.device(\"cpu\")\n",
    "    model = inferer.load_model(model_dir, checkpoint_step)\n",
    "    dataset = registry.construct('dataset', inferer.config['data']['val'])\n",
    "\n",
    "    for _, schema in dataset.schemas.items():\n",
    "        model.preproc.enc_preproc._preprocess_schema(schema)\n",
    "    \n",
    "    _ret_dict = {\n",
    "        'model': model,\n",
    "        'dataset': dataset,\n",
    "        'inferer': inferer,\n",
    "    }\n",
    "    \n",
    "    return _ret_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0/model_checkpoint-00040000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|██████████| 166/166 [00:01<00:00, 124.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dataset', 'inferer'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_sql_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                  exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-run.jsonnet',\n",
    "                                  model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                  checkpoint_step=40000)\n",
    "rat_sql_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_sql_asr_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                      exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-ASR-run.jsonnet',\n",
    "                                      model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_ASR_run/ASR,bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                      checkpoint_step=40000)\n",
    "rat_sql_asr_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_sql_mixed_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                        exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-mixed-run.jsonnet',\n",
    "                                        model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_mixed_run/mixed,bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                        checkpoint_step=40000)\n",
    "rat_sql_mixed_model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dicts = {\n",
    "    'orig': rat_sql_model_dict,\n",
    "#     'asr': rat_sql_asr_model_dict,\n",
    "#     'mixed': rat_sql_mixed_model_dict,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question(q, db_id, model_dict=model_dicts['orig']):\n",
    "    model = model_dict['model']\n",
    "    dataset = model_dict['dataset']\n",
    "    inferer = model_dict['inferer']\n",
    "    \n",
    "    spider_schema = dataset.schemas[db_id]\n",
    "    data_item = SpiderItem(\n",
    "        text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly\n",
    "        code=None,\n",
    "        schema=spider_schema,\n",
    "        orig_schema=spider_schema.orig,\n",
    "        orig={\"question\": q}\n",
    "    )\n",
    "    model.preproc.clear_items()\n",
    "    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)\n",
    "    preproc_data = enc_input, None\n",
    "    with torch.no_grad():\n",
    "        return inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_question': 'how many singers do we have?',\n",
       "  'model_output': {'_type': 'sql',\n",
       "   'select': {'_type': 'select',\n",
       "    'is_distinct': False,\n",
       "    'aggs': [{'_type': 'agg',\n",
       "      'agg_id': {'_type': 'Count'},\n",
       "      'val_unit': {'_type': 'Column',\n",
       "       'col_unit1': {'_type': 'col_unit',\n",
       "        'agg_id': {'_type': 'NoneAggOp'},\n",
       "        'col_id': 0,\n",
       "        'is_distinct': False}}}]},\n",
       "   'sql_where': {'_type': 'sql_where'},\n",
       "   'sql_groupby': {'_type': 'sql_groupby'},\n",
       "   'sql_orderby': {'_type': 'sql_orderby', 'limit': False},\n",
       "   'sql_ieu': {'_type': 'sql_ieu'},\n",
       "   'from': {'_type': 'from',\n",
       "    'table_units': [{'_type': 'Table', 'table_id': 1}]}},\n",
       "  'inferred_code': 'SELECT Count(*) FROM singer',\n",
       "  'score': -0.00011539317730324683}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\", model_dict=model_dicts['mixed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\",\n",
    "         model_dict=model_dicts['mixed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_path, 'r') as f:\n",
    "    dev_dataset = json.load(f)\n",
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dev_dataset[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(d['question'], d['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sql_list = []\n",
    "\n",
    "for d in tqdm(dev_dataset):\n",
    "    pred = question(d['question'], d['db_id'])[0]\n",
    "    pred_sql_list.append(pred['inferred_code'])\n",
    "\n",
    "len(pred_sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_path = './output/dev_output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dev_pred_path, 'w') as f:\n",
    "#     for pred in pred_sql_list:\n",
    "#         f.write(pred + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict SQL given transcribed dataset JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'train'\n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon.json'.format(DATASET)\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon_RatsqlPredicted.json'.format(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for human test \n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon.json'\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon_RatsqlPredicted.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_dataset_path, 'r') as f:\n",
    "    asr_dataset = json.load(f)\n",
    "len(asr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, d in tqdm(enumerate(asr_dataset), total=len(asr_dataset)):\n",
    "    if 'ratsql_pred_sql' in d:\n",
    "        continue\n",
    "        \n",
    "    pred = Question(d['question'], d['db_id'])\n",
    "    if len(pred) == 0:\n",
    "        print('{}: question({}, {}) failed'.format(i, d['question'], d['db_id']))\n",
    "        d['ratsql_pred_sql'] = ''\n",
    "    else:\n",
    "        d['ratsql_pred_sql'] = pred[0]['inferred_code']\n",
    "\n",
    "asr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = question(d['question'], d['db_id'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_dataset_path, 'w') as f:\n",
    "#     json.dump(asr_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict & Evaluate SQL given rewriter output / original / ASR cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1.0, 0), (0, 0.5, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateSQL(pred_str='SELECT * FROM singer WHERE name = \"Joe Sharp\"',\n",
    "            gold_str='SELECT * FROM singer WHERE name = \"DEF\"',\n",
    "            db='concert_singer'), \\\n",
    "EvaluateSQL(pred_str='SELECT country FROM singer WHERE name = \"ABC\"',\n",
    "            gold_str='SELECT country FROM singer WHERE name = \"DEF\" ORDER BY age LIMIT 1',\n",
    "            db='concert_singer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human test \n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603a05a5e87b48a6aa2c8b8e91546f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Just using the 1st ASR candidate, no correction \n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['question'], _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7274\n",
      "avg_exact = 0.4552\n",
      "avg_exec = 0.3327\n",
      "BLEU = 0.8010\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bdeeefe86d46a4a5c97dfa7967a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.397                0.287                0.352                0.338                0.333               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.625                0.446                0.385                0.275                0.455               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.846                0.667                0.769                0.738                0.739               \n",
      "select(no AGG)       0.882                0.671                0.769                0.738                0.750               \n",
      "where                0.614                0.600                0.554                0.318                0.548               \n",
      "where(no OP)         0.667                0.609                0.643                0.432                0.599               \n",
      "group(no Having)     0.867                0.753                0.850                0.800                0.789               \n",
      "group                0.800                0.714                0.800                0.771                0.748               \n",
      "order                0.889                0.535                0.800                0.921                0.750               \n",
      "and/or               1.000                0.945                0.956                0.861                0.948               \n",
      "IUEN                 0.000                0.000                0.167                0.214                0.167               \n",
      "keywords             0.892                0.826                0.856                0.762                0.833               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.846                0.667                0.769                0.738                0.739               \n",
      "select(no AGG)       0.882                0.671                0.769                0.738                0.750               \n",
      "where                0.538                0.616                0.620                0.292                0.542               \n",
      "where(no OP)         0.585                0.625                0.720                0.396                0.593               \n",
      "group(no Having)     0.812                0.763                0.773                0.737                0.763               \n",
      "group                0.750                0.724                0.727                0.711                0.724               \n",
      "order                1.000                0.639                0.774                0.921                0.796               \n",
      "and/or               0.963                0.987                0.989                0.958                0.977               \n",
      "IUEN                 0.000                0.000                0.125                0.188                0.150               \n",
      "keywords             0.831                0.822                0.846                0.762                0.819               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.846                0.667                0.769                0.738                0.739               \n",
      "select(no AGG)       0.882                0.671                0.769                0.738                0.750               \n",
      "where                0.574                0.608                0.585                0.304                0.545               \n",
      "where(no OP)         0.623                0.617                0.679                0.413                0.596               \n",
      "group(no Having)     0.839                0.758                0.810                0.767                0.776               \n",
      "group                0.774                0.719                0.762                0.740                0.736               \n",
      "order                0.941                0.582                0.787                0.921                0.773               \n",
      "and/or               0.981                0.966                0.972                0.907                0.962               \n",
      "IUEN                 1.000                1.000                0.143                0.200                0.158               \n",
      "keywords             0.860                0.824                0.851                0.762                0.826               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.825                0.687                0.735                0.676                0.727               \n",
      "\n",
      "0.625\t0.44583333333333336\t0.38461538461538464\t0.275\t0.45521023765996343\n",
      "0.8245652427637721\t0.6865574124949128\t0.7350149850149849\t0.6758904637029636\t0.7273715772344649\n",
      "0.39705882352941174\t0.2875\t0.3516483516483517\t0.3375\t0.3327239488117002\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the predictions \n",
    "# Orig\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/Assembly/First-cands.json'\n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/Assembly/First-cands.json'\n",
    "# Mixed\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/Assembly/First-cands.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a4a806790a4b25801a23815824e379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['gold_question'], _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.8316\n",
      "avg_exact = 0.6234\n",
      "avg_exec = 0.4095\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5165faacc7c346d086f43c6a76636c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.463                0.375                0.418                0.412                0.410               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.787                0.658                0.462                0.425                0.623               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.926                0.812                0.934                0.838                0.864               \n",
      "select(no AGG)       0.963                0.820                0.934                0.838                0.877               \n",
      "where                0.776                0.744                0.627                0.426                0.677               \n",
      "where(no OP)         0.821                0.752                0.745                0.553                0.734               \n",
      "group(no Having)     0.812                0.797                0.810                0.806                0.803               \n",
      "group                0.750                0.757                0.762                0.778                0.762               \n",
      "order                1.000                0.650                0.781                0.947                0.805               \n",
      "and/or               1.000                0.958                0.955                0.863                0.954               \n",
      "IUEN                 0.000                0.000                0.350                0.500                0.395               \n",
      "keywords             0.945                0.902                0.890                0.850                0.899               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.926                0.808                0.934                0.838                0.863               \n",
      "select(no AGG)       0.963                0.817                0.934                0.838                0.876               \n",
      "where                0.800                0.777                0.640                0.417                0.695               \n",
      "where(no OP)         0.846                0.786                0.760                0.542                0.753               \n",
      "group(no Having)     0.812                0.776                0.773                0.763                0.776               \n",
      "group                0.750                0.737                0.727                0.737                0.737               \n",
      "order                1.000                0.722                0.806                0.947                0.841               \n",
      "and/or               0.963                0.991                0.966                0.972                0.977               \n",
      "IUEN                 0.000                0.000                0.292                0.500                0.375               \n",
      "keywords             0.966                0.907                0.890                0.850                0.905               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.926                0.810                0.934                0.838                0.864               \n",
      "select(no AGG)       0.963                0.818                0.934                0.838                0.876               \n",
      "where                0.788                0.760                0.634                0.421                0.686               \n",
      "where(no OP)         0.833                0.769                0.752                0.547                0.743               \n",
      "group(no Having)     0.812                0.787                0.791                0.784                0.789               \n",
      "group                0.750                0.747                0.744                0.757                0.749               \n",
      "order                1.000                0.684                0.794                0.947                0.823               \n",
      "and/or               0.981                0.974                0.960                0.914                0.965               \n",
      "IUEN                 1.000                1.000                0.318                0.500                0.385               \n",
      "keywords             0.956                0.904                0.890                0.850                0.902               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.914                0.816                0.810                0.763                0.832               \n",
      "\n",
      "0.7867647058823529\t0.6583333333333333\t0.46153846153846156\t0.425\t0.623400365630713\n",
      "0.9140552054154994\t0.8161431623931626\t0.810234804877662\t0.7633404095904094\t0.83178147517727\n",
      "0.4632352941176471\t0.375\t0.4175824175824176\t0.4125\t0.40950639853747717\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_subset = random.sample(test_dataset, k=100)\n",
    "\n",
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in random_subset],\n",
    "                 glist=[d[0]['query'] for d in random_subset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in random_subset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRIDGE results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TabularSemanticParsing/server/spider.bridge.lstm.meta.ts.ppl-0.85.2.dn.eo.feat.bert-base-uncased.xavier-768-400-400-8-4-0.0005-inv-sqr-0.0005-4000-6e-05-inv-sqr-3e-05-4000-0.1-0.3-0.0-0.0-1-8-0.0-0.0-res-0.2-0.0-ff-0.4-0.0.210126-135032.7kf9/predictions.16.ASR.dev.txt'\n",
    "gold_query_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev_gold.sql'\n",
    "test_index_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/index_test.txt'\n",
    "# db_id_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/db_id.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1034, 1034, 547)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(bridge_pred_path, 'r') as f:\n",
    "    bridge_preds = f.read().strip().split('\\n')\n",
    "with open(gold_query_path, 'r') as f:\n",
    "    _lines = [l.split('\\t') for l in f.read().strip().split('\\n')]\n",
    "    gold_queries = [l[0] for l in _lines]\n",
    "    db_ids = [l[1] for l in _lines]\n",
    "# with open(db_id_path, 'r') as f:\n",
    "#     db_ids = f.read().strip().split('\\n')\n",
    "with open(test_index_path, 'r') as f:\n",
    "    test_indices = [int(i) for i in f.read().strip().split('\\n')]\n",
    "    \n",
    "len(bridge_preds), len(gold_queries), len(db_ids), len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e1b7f2bde42629b9834007d10e231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=547.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "eval_err_num:2\n",
      "eval_err_num:3\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                136                  240                  91                   80                   547                 \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.654                0.442                0.451                0.325                0.479               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.750                0.450                0.429                0.287                0.497               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.882                0.679                0.824                0.762                0.767               \n",
      "select(no AGG)       0.904                0.688                0.824                0.762                0.776               \n",
      "where                0.812                0.626                0.609                0.413                0.631               \n",
      "where(no OP)         0.859                0.652                0.652                0.500                0.675               \n",
      "group(no Having)     0.625                0.788                0.680                0.781                0.748               \n",
      "group                0.625                0.727                0.680                0.750                0.712               \n",
      "order                0.889                0.579                0.828                0.886                0.766               \n",
      "and/or               1.000                0.950                0.956                0.872                0.952               \n",
      "IUEN                 0.000                0.000                0.579                0.538                0.529               \n",
      "keywords             0.899                0.885                0.837                0.772                0.860               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.882                0.671                0.824                0.762                0.762               \n",
      "select(no AGG)       0.904                0.679                0.824                0.762                0.771               \n",
      "where                0.800                0.643                0.560                0.396                0.622               \n",
      "where(no OP)         0.846                0.670                0.600                0.479                0.665               \n",
      "group(no Having)     0.625                0.684                0.773                0.658                0.684               \n",
      "group                0.625                0.632                0.773                0.632                0.651               \n",
      "order                1.000                0.611                0.774                0.816                0.752               \n",
      "and/or               0.993                0.991                1.000                0.944                0.987               \n",
      "IUEN                 0.000                0.000                0.458                0.438                0.450               \n",
      "keywords             0.899                0.864                0.791                0.762                0.840               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.882                0.675                0.824                0.762                0.764               \n",
      "select(no AGG)       0.904                0.683                0.824                0.762                0.774               \n",
      "where                0.806                0.634                0.583                0.404                0.626               \n",
      "where(no OP)         0.853                0.661                0.625                0.489                0.670               \n",
      "group(no Having)     0.625                0.732                0.723                0.714                0.715               \n",
      "group                0.625                0.676                0.723                0.686                0.680               \n",
      "order                0.941                0.595                0.800                0.849                0.759               \n",
      "and/or               0.996                0.970                0.978                0.907                0.969               \n",
      "IUEN                 1.000                1.000                0.512                0.483                0.486               \n",
      "keywords             0.899                0.875                0.814                0.767                0.850               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.884                0.708                0.735                0.683                0.753               \n",
      "\n",
      "Exact: 0.75\t0.45\t0.42857142857142855\t0.2875\t0.49725776965265084\n",
      "Exec: 0.6544117647058824\t0.44166666666666665\t0.45054945054945056\t0.325\t0.4789762340036563\n",
      "Partial: 0.8836285650623887\t0.7084778300403303\t0.735301023762562\t0.68322823010323\t0.7527949097418934\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=[bridge_preds[idx] for idx in test_indices],\n",
    "                 glist=[gold_queries[idx] for idx in test_indices],\n",
    "                 db_id_list=[db_ids[idx] for idx in test_indices],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80340cac1d7c4ab38c4b78dd31897ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1034.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_err_num:1\n",
      "eval_err_num:2\n",
      "eval_err_num:3\n",
      "eval_err_num:4\n",
      "\n",
      "                     easy                 medium               hard                 extra                all                 \n",
      "count                250                  440                  174                  170                  1034                \n",
      "=====================   EXECUTION ACCURACY     =====================\n",
      "execution            0.652                0.439                0.529                0.276                0.479               \n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.748                0.468                0.500                0.253                0.506               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.871                0.666                0.851                0.612                0.738               \n",
      "select(no AGG)       0.900                0.670                0.862                0.612                0.749               \n",
      "where                0.821                0.651                0.619                0.384                0.627               \n",
      "where(no OP)         0.868                0.677                0.655                0.475                0.674               \n",
      "group(no Having)     0.650                0.699                0.714                0.742                0.709               \n",
      "group                0.650                0.650                0.714                0.727                0.681               \n",
      "order                0.741                0.641                0.865                0.797                0.754               \n",
      "and/or               1.000                0.959                0.971                0.896                0.961               \n",
      "IUEN                 0.000                0.000                0.581                0.391                0.447               \n",
      "keywords             0.875                0.862                0.839                0.661                0.821               \n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.868                0.661                0.851                0.612                0.735               \n",
      "select(no AGG)       0.896                0.666                0.862                0.612                0.746               \n",
      "where                0.806                0.680                0.565                0.388                0.626               \n",
      "where(no OP)         0.852                0.708                0.598                0.480                0.672               \n",
      "group(no Having)     0.650                0.656                0.769                0.620                0.662               \n",
      "group                0.650                0.611                0.769                0.608                0.636               \n",
      "order                0.909                0.667                0.763                0.778                0.751               \n",
      "and/or               0.992                0.986                0.994                0.930                0.980               \n",
      "IUEN                 0.000                0.000                0.595                0.250                0.436               \n",
      "keywords             0.887                0.855                0.810                0.641                0.809               \n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.870                0.664                0.851                0.612                0.736               \n",
      "select(no AGG)       0.898                0.668                0.862                0.612                0.747               \n",
      "where                0.813                0.665                0.591                0.386                0.627               \n",
      "where(no OP)         0.860                0.692                0.625                0.477                0.673               \n",
      "group(no Having)     0.650                0.677                0.741                0.676                0.685               \n",
      "group                0.650                0.630                0.741                0.662                0.658               \n",
      "order                0.816                0.654                0.811                0.787                0.753               \n",
      "and/or               0.996                0.972                0.982                0.913                0.970               \n",
      "IUEN                 1.000                1.000                0.588                0.305                0.442               \n",
      "keywords             0.881                0.858                0.825                0.651                0.815               \n",
      "================   PARTIAL MATCHING SUMMARY SCORE    ================\n",
      "partial_summary      0.876                0.698                0.767                0.591                0.735               \n",
      "\n",
      "Exact: 0.748\t0.4681818181818182\t0.5\t0.2529411764705882\t0.5058027079303675\n",
      "Exec: 0.652\t0.43863636363636366\t0.5287356321839081\t0.27647058823529413\t0.4787234042553192\n",
      "Partial: 0.8756383838383834\t0.6980433896911165\t0.7674289152737428\t0.5908789419012966\t0.7350393992306947\n"
     ]
    }
   ],
   "source": [
    "EvaluateSQL_full(plist=bridge_preds,\n",
    "                 glist=gold_queries,\n",
    "                 db_id_list=db_ids,\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '4.0.1.0_laststep'\n",
    "\n",
    "end2end_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 1034)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(end2end_pred_path, 'r') as f:\n",
    "    end2end_preds = [json.loads(l) for l in f]\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "    \n",
    "len(end2end_preds), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4baca41910f4195b88230fdf342f499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3075), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_str: SELECT first_name ,  last_name FROM players ORDER BY birth_date\n",
      "p_str: SELECT matches.winner_name, matches.winner_name FROM matches ORDER BY matches.winner_name Asc\n",
      "g_str: SELECT first_name ,  last_name FROM players ORDER BY birth_date\n",
      "p_str: SELECT DISTINCT matches.winner_name FROM players JOIN matches ON players.player_id = matches.loser_id ORDER BY players.last_name Asc\n",
      "SELECT Count(DISTINCT conductor.Conductor_ID) FROM conductor JOIN orchestra ON conductor.Conductor_ID = orchestra.Conductor_ID AND conductor.Conductor_ID = orchestra.Conductor_ID JOIN performance ON orchestra.Orchestra_ID = performance.Orchestra_ID AND orchestra.Orchestra_ID = performance.Orchestra_ID JOIN show ON performance.Performance_ID = show.Performance_ID WHERE show.If_first_show = 'terminal' OR performance.Official_ratings_(millions) = 'terminal'\n",
      "SELECT COUNT(*) FROM orchestra WHERE Major_Record_Format  =  \"CD\" OR Major_Record_Format  =  \"DVD\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "_seen_ids = set()\n",
    "_first_cand_preds = []\n",
    "\n",
    "for i, p in tqdm(enumerate(end2end_preds), total=len(end2end_preds)):\n",
    "\n",
    "    # p = rewriter_ILM_preds[pred_idx]\n",
    "    # _o_idx = c['original_id']\n",
    "    # o = orig_dev_dataset[_o_idx]\n",
    "    # assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    # assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _o_idx = p['original_id']\n",
    "    if _o_idx in _seen_ids:\n",
    "        continue\n",
    "    else:\n",
    "        _seen_ids.add(_o_idx)\n",
    "        \n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "    _pred_sql = p['pred_sql']\n",
    "    _gold_sql = p['gold_sql']\n",
    "    assert _gold_sql == o['query'], (_gold_sql, o['query'])\n",
    "    \n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    # Save prediction results \n",
    "    p['score'] = _score\n",
    "    p['exact'] = _exact\n",
    "    p['exec'] = _exec\n",
    "    \n",
    "    _first_cand_preds.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.3425\n",
      "avg_exact = 0.0146\n",
      "avg_exec = 0.0256\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([p['score'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exact_1st = sum([p['exact'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exec_1st = sum([p['exec'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '1.14.1.2'\n",
    "HUMAN_TEST = False\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggreg with rewriter cands \n",
    "\n",
    "RERANKER_VERSION = '1.10.0.2'\n",
    "REWRITER_VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "HUMAN_TEST = True\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/test_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-humantest-yshao-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/human_test_yshao_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(reranker_pred_path, 'r') as f:\n",
    "    reranker_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(reranker_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['score_preds', 'question', 'original_id']),\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance']))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_preds[0].keys(), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9994f50f0dc74926b9ca1cc60bcfb1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Find the cand with highest score \n",
    "    d_preds = []\n",
    "    for _ in d:\n",
    "        p = reranker_preds[pred_idx]\n",
    "        d_preds.append(p)\n",
    "        pred_idx += 1\n",
    "    \n",
    "    _c_idx = np.argmax([p['score_preds'] for p in d_preds])\n",
    "    \n",
    "    c = d[_c_idx]\n",
    "    p = d_preds[_c_idx]\n",
    "    \n",
    "    for _c in d:\n",
    "        _c['is_reranker_selection'] = False\n",
    "    c['is_reranker_selection'] = True\n",
    "    \n",
    "    # Use the selected cand to proceed \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    _question = c['question']\n",
    "\n",
    "    if c['ratsql_pred_sql'] is not None:\n",
    "        # If SQL is already predicted, no need to predict again \n",
    "        _pred_sql = c['ratsql_pred_sql']\n",
    "    else:\n",
    "        # If SQL not yet predicted, do it \n",
    "        _pred_sql = Question(_question, _db_id)[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "#     # Save the taggerILM raw outputs, for later aggregation \n",
    "#     c['pred_tags'] = p['rewriter_tags']\n",
    "#     c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "#     c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "    \n",
    "    # Save prediction results \n",
    "    # c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    # p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "    c['exec'] = p['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7599 (std = 0.2977)\n",
      "avg_exact = 0.5027\n",
      "avg_exec = 0.3620\n",
      "BLEU = 0.7853\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([c['exact'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([c['exec'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[c['pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection']],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non ASR\n",
    "if not HUMAN_TEST:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "else:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "    \n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tagger accuracy \n",
    "VERSION = '2.12.7.2t'\n",
    "\n",
    "predict_fname = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3075"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(predict_fname, 'r') as f:\n",
    "    predicts = [json.loads(l) for l in f]\n",
    "len(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall tagger accuracy = 38648/41776 = 0.9251\n",
      "Tagger P = 0.7118, R = 0.7006, F1 = 0.7062\n"
     ]
    }
   ],
   "source": [
    "correct_tags = 0\n",
    "total_tags = 0\n",
    "\n",
    "non_O_matches = 0\n",
    "non_O_preds = 0\n",
    "non_O_golds = 0\n",
    "\n",
    "for p in predicts:\n",
    "    _pred_tags = p['tags_prediction']\n",
    "    _gold_tags = [t for t in p['gold_tags'] if t != 'O']\n",
    "    assert len(_pred_tags) == len(_gold_tags)\n",
    "    \n",
    "    _cor = sum([_p == _g for _p, _g in zip(_pred_tags, _gold_tags)])\n",
    "    _total = len(_pred_tags)\n",
    "    assert -1e-8 < _cor / _total - p['tags_accuracy'] < 1e-8, f\"{_cor / _total}, {p['tags_accuracy']}\"\n",
    "    \n",
    "    for _p, _g in zip(_pred_tags, _gold_tags):\n",
    "        if _p != 'O-KEEP':\n",
    "            non_O_preds += 1\n",
    "        if _g != 'O-KEEP':\n",
    "            non_O_golds += 1\n",
    "        if _p == _g and _p != 'O-KEEP':\n",
    "            non_O_matches += 1\n",
    "    \n",
    "    correct_tags += _cor\n",
    "    total_tags += _total\n",
    "\n",
    "tagger_prec = non_O_matches / non_O_preds\n",
    "tagger_recall = non_O_matches / non_O_golds\n",
    "tagger_F1 = 2 * tagger_prec * tagger_recall / (tagger_prec + tagger_recall)\n",
    "    \n",
    "print(f'Overall tagger accuracy = {correct_tags}/{total_tags} = {correct_tags / total_tags:.4f}')\n",
    "print(f'Tagger P = {tagger_prec:.4f}, R = {tagger_recall:.4f}, F1 = {tagger_F1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full tagger-ILM\n",
    "\n",
    "VERSION = '2.12.7.2t-2.12.7.2i'\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    if ASR == 'AssemblyAI':\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "    rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e52230e7734d73af0b86cfa1f45e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['and'], ['named', '``'], ['chervil', \"''\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['airports', \"'\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['document'], ['id'], ['customer', '``'], ['CV', \"''\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['``', 'CLE'], [\"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'B-DEL', 'L-DEL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['titled', '``'], ['January', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['contain'], ['document', '``'], ['Murray', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Return'], ['services']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'B-EDIT', 'I-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['are'], ['not', '``'], ['USA', \"''\"], ['?']]\n",
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_ILM_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "    # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "    \n",
    "    _tags = p['rewriter_tags']\n",
    "    _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    _question_toks = c['question_toks']\n",
    "\n",
    "    _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "    _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    # Save the taggerILM raw outputs, for later aggregation \n",
    "    c['pred_tags'] = p['rewriter_tags']\n",
    "    c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "    c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "    \n",
    "    # Save prediction results \n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "    c['exec'] = p['exec'] = _exec\n",
    "\n",
    "    _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_rewritten_question_toks)\n",
    "    \n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7535 (std = 0.2996)\n",
      "avg_exact = 0.4936\n",
      "avg_exec = 0.3601\n",
      "BLEU = 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non ASR\n",
    "if not HUMAN_TEST:\n",
    "    if ASR == 'AssemblyAI':\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "else:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "# Mixed\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from predicted file (only 1st cand is predicted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.6.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using EvaluateSQL_full \n",
    "\n",
    "# tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "# kmaps = evaluation.build_foreign_key_map_from_json(tables_json)\n",
    "\n",
    "plist = [d[0]['pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reranker output file \n",
    "VERSION = '1.10.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [c['ratsql_pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection'] == 1]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [d[0]['ratsql_pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using all ASR candidates (no longer in use)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']\n",
    "        _tags = p['tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation process with oracle tags (no longer in use for version>=2.3.0)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _tags = p['gold_tags']\n",
    "        _rewrite_seq = p['oracle_tags_rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question'] = _rewritten_question\n",
    "        c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql'] = _pred_sql\n",
    "        c['oracle_tags_score'] = p['oracle_tags_score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['oracle_tags_score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['oracle_tags_score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_oracle_avg_1st = sum([d[0]['oracle_tags_score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_oracle_std_1st = np.std([d[0]['oracle_tags_score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_oracle_avg_1st, _oracle_std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['tag_prediction'] = list(zip(p['question'].split(' '), p['tags_prediction']))\n",
    "        _pred_c['rewrite_seq'] = []\n",
    "        for t in p['rewrite_seq_prediction']:\n",
    "            _pred_c['rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['rewritten_question'] = p['rewritten_question']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_tags'] = list(zip(p['question'].split(' '), p['gold_tags']))\n",
    "        _pred_c['oracle_tags_rewrite_seq'] = []\n",
    "        for t in p['oracle_tags_rewrite_seq_prediction']:\n",
    "            _pred_c['oracle_tags_rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question']\n",
    "        _pred_c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql']\n",
    "        _pred_c['oracle_tags_score'] = p['oracle_tags_score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test-prediction-{}.json'.format(VERSION), 'w') as f:\n",
    "    json.dump(test_pred_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset file with predictions \n",
    "\n",
    "with open('./output/pred-{}.json'.format(VERSION), 'r') as f:\n",
    "    test_pred_dataset = json.load(f)\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis \n",
    "orig_dev_preds_path = './output/dev_output.txt'\n",
    "\n",
    "with open(orig_dev_preds_path, 'r') as f:\n",
    "    orig_dev_preds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "len(orig_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for d in test_dataset[5::40]:\n",
    "#     print('DB:', d[0]['db_id'])\n",
    "#     print('ASR question:\\t\\t', d[0]['question'])\n",
    "#     print('Rewritten question:\\t', d[0]['rewritten_question'])\n",
    "#     print('Gold question:\\t\\t', d[0]['gold_question'])\n",
    "#     print('ASR-q Pred SQL:\\t\\t', d[0]['ratsql_pred_sql'])\n",
    "#     print('Rewritten-q Pred SQL:\\t', d[0]['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', orig_dev_preds[d[0]['original_id']])\n",
    "#     print('Gold SQL:\\t\\t', d[0]['query'])\n",
    "#     print('Score:', d[0]['score'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_samples = [c for d in test_pred_dataset for c in d]\n",
    "for i, c in list(enumerate(test_pred_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['ASR_question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "    print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['gold_sql'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = [8, 96, 272, 448, 1416, 1592, 1680, 1856, 2120, 2296, 2384, 2560, 2824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (Amazon): VERSION = 3.12.1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbadc38d54ef4598b38e1fb716ad31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSION 3.12.1.1:\n",
      "avg = 0.7410 (std = 0.3078)\n",
      "avg_exact = 0.4826\n",
      "avg_exec = 0.3638\n",
      "BLEU = 0.8532\n",
      "\n",
      "Evaluating (Amazon): VERSION = 3.12.1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb02e39911f48f08c1f045f67cbcb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.12.1.2:\n",
      "avg = 0.7478 (std = 0.3037)\n",
      "avg_exact = 0.4899\n",
      "avg_exec = 0.3675\n",
      "BLEU = 0.8566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Batch evaluating \n",
    "\n",
    "VERSION_LIST = ['3.12.1.1', '3.12.1.2']\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    print(f'Evaluating ({ASR}){\" (human test)\" if HUMAN_TEST else \"\"}: VERSION = {VERSION}')\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        # human test \n",
    "        rewriter_s2s_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    \n",
    "    with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "        rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "        \n",
    "    # Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    \n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags']\n",
    "        # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        # _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "\n",
    "        if _rewritten_question == '':\n",
    "            print(f'_rewritten_question is empty')\n",
    "            _pred_sql = ''\n",
    "            _gold_sql = c['query']\n",
    "            _exact = _score = _exec = 0\n",
    "        else:\n",
    "            _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "            _gold_sql = c['query']\n",
    "            _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "        \n",
    "        # For BLEU \n",
    "        _rewritten_question_toks = [_t.lower() for _t in p['s2s_prediction']]\n",
    "        _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "    \n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    \n",
    "    print(f'VERSION {VERSION}:')\n",
    "    print(f'avg = {_avg_1st:.4f} (std = {_std_1st:.4f})')\n",
    "    print(f'avg_exact = {_avg_exact_1st:.4f}')\n",
    "    print(f'avg_exec = {_avg_exec_1st:.4f}')\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print()\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "        \n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single version, not used \n",
    "\n",
    "VERSION = '3.3.0.0'\n",
    "\n",
    "rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "    rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_preds[3::300]:\n",
    "    print(p['question'])\n",
    "    print(' '.join(p['s2s_prediction']))\n",
    "    print(' '.join(p['gold_rewrite_seq_s2s'][1:-1]))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags']\n",
    "    # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    # _question_toks = c['question_toks']\n",
    "\n",
    "    # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "    \n",
    "    if _rewritten_question == '':\n",
    "        print(f'_rewritten_question is empty')\n",
    "        _pred_sql = ''\n",
    "        _gold_sql = c['query']\n",
    "        _score = 0\n",
    "    else:\n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "\n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actual (full) evaluation process \n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        pred_idx += 1\n",
    "        if 'score' in c:\n",
    "            continue  # already inferred  \n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "        _pred_result = Question(_rewritten_question, _db_id)\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        \n",
    "        if len(_pred_result) == 0:\n",
    "            print(_db_id, _rewritten_question, '-- no predictiction')\n",
    "            _pred_sql = ''\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = _pred_result[0]['inferred_code']\n",
    "            _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_preds]) / len(rewriter_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['rewritten_question'] = p['s2s_prediction']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [c for d in test_dataset for c in d]\n",
    "for i, c in list(enumerate(test_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['query'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_ILM_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dev_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite_seq postprocessing, to get the rewritten question \n",
    "\n",
    "_idx = 154\n",
    "\n",
    "_tags = rewriter_ILM_preds[_idx]['tags_prediction']\n",
    "_rewrite_seq = rewriter_ILM_preds[_idx]['rewrite_seq_prediction']\n",
    "_question_toks = rewriter_ILM_preds[_idx]['question'].split(' ')\n",
    "_tags, _rewrite_seq, _question_toks\n",
    "\n",
    "postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'concert_singer'\n",
    "g_str = 'SELECT count(*) FROM singer'\n",
    "p_str = \"SELECT Count(DISTINCT singer.Name) FROM singer WHERE singer.Name = 'terminal'\"\n",
    "\n",
    "db, p_str, g_str, EvaluateSQL(p_str, g_str, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 41112)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/train/train_reranker.json') as f:\n",
    "    _train = json.load(f)\n",
    "len(_train), sum([len(d) for d in _train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 623.44 MiB, increment: -2.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
