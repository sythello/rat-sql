{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import _jsonnet\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/third_party/wikisql'))\n",
    "sys.path.append(os.path.abspath('/Users/mac/Desktop/syt/Deep-Learning/Projects-M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.datasets.spider import SpiderItem\n",
    "from ratsql.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeakQL.SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_config_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-run.jsonnet'\n",
    "exp_config = json.loads(_jsonnet.evaluate_file(exp_config_path))\n",
    "exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql'\n",
    "model_config_path = os.path.join(root_dir, exp_config[\"model_config\"])\n",
    "model_config_args = exp_config.get(\"model_config_args\")\n",
    "model_config_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infer_config = json.loads(_jsonnet.evaluate_file(model_config_path, tla_codes={'args': json.dumps(model_config_args)}))\n",
    "infer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_step = 40000\n",
    "model_dir = '/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0'\n",
    "\n",
    "inferer = Inferer(infer_config)\n",
    "inferer.device = torch.device(\"cpu\")\n",
    "model = inferer.load_model(model_dir, checkpoint_step)\n",
    "dataset = registry.construct('dataset', inferer.config['data']['val'])\n",
    "\n",
    "for _, schema in dataset.schemas.items():\n",
    "    model.preproc.enc_preproc._preprocess_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Rat_sql(root_dir,\n",
    "                 exp_config_path,\n",
    "                 model_dir,\n",
    "                 checkpoint_step=40000):\n",
    "\n",
    "    exp_config = json.loads(_jsonnet.evaluate_file(exp_config_path))\n",
    "    \n",
    "    model_config_path = os.path.join(root_dir, exp_config[\"model_config\"])\n",
    "    model_config_args = exp_config.get(\"model_config_args\")\n",
    "    \n",
    "    infer_config = json.loads(_jsonnet.evaluate_file(model_config_path, tla_codes={'args': json.dumps(model_config_args)}))\n",
    "\n",
    "    inferer = Inferer(infer_config)\n",
    "    inferer.device = torch.device(\"cpu\")\n",
    "    model = inferer.load_model(model_dir, checkpoint_step)\n",
    "    dataset = registry.construct('dataset', inferer.config['data']['val'])\n",
    "\n",
    "    for _, schema in dataset.schemas.items():\n",
    "        model.preproc.enc_preproc._preprocess_schema(schema)\n",
    "    \n",
    "    _ret_dict = {\n",
    "        'model': model,\n",
    "        'dataset': dataset,\n",
    "        'inferer': inferer,\n",
    "    }\n",
    "    \n",
    "    return _ret_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0/model_checkpoint-00040000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|██████████| 166/166 [00:01<00:00, 132.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dataset', 'inferer'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_sql_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                  exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-run.jsonnet',\n",
    "                                  model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_run/bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                  checkpoint_step=40000)\n",
    "rat_sql_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_sql_asr_model_dict = Load_Rat_sql(root_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql',\n",
    "                                      exp_config_path='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/experiments/spider-glove-ASR-run.jsonnet',\n",
    "                                      model_dir='/Users/mac/Desktop/syt/Deep-Learning/Repos/rat-sql/logdir/glove_ASR_run/ASR,bs=20,lr=7.4e-04,end_lr=0e0,att=0',\n",
    "                                      checkpoint_step=40000)\n",
    "rat_sql_asr_model_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question(q, db_id, model_dict=rat_sql_model_dict):\n",
    "    model = model_dict['model']\n",
    "    dataset = model_dict['dataset']\n",
    "    inferer = model_dict['inferer']\n",
    "    \n",
    "    spider_schema = dataset.schemas[db_id]\n",
    "    data_item = SpiderItem(\n",
    "        text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly\n",
    "        code=None,\n",
    "        schema=spider_schema,\n",
    "        orig_schema=spider_schema.orig,\n",
    "        orig={\"question\": q}\n",
    "    )\n",
    "    model.preproc.clear_items()\n",
    "    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)\n",
    "    preproc_data = enc_input, None\n",
    "    with torch.no_grad():\n",
    "        return inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question(\"how many singers do we have?\", \"concert_singer\", model_dict=rat_sql_asr_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Question(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\",\n",
    "         model_dict=rat_sql_asr_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_path, 'r') as f:\n",
    "    dev_dataset = json.load(f)\n",
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dev_dataset[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(d['question'], d['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sql_list = []\n",
    "\n",
    "for d in tqdm(dev_dataset):\n",
    "    pred = question(d['question'], d['db_id'])[0]\n",
    "    pred_sql_list.append(pred['inferred_code'])\n",
    "\n",
    "len(pred_sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_path = './output/dev_output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dev_pred_path, 'w') as f:\n",
    "#     for pred in pred_sql_list:\n",
    "#         f.write(pred + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict SQL given transcribed dataset JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'train'\n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon.json'.format(DATASET)\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/{0}/{0}_asr_amazon_RatsqlPredicted.json'.format(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for human test \n",
    "input_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon.json'\n",
    "output_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_asr_amazon_RatsqlPredicted.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_dataset_path, 'r') as f:\n",
    "    asr_dataset = json.load(f)\n",
    "len(asr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, d in tqdm(enumerate(asr_dataset), total=len(asr_dataset)):\n",
    "    if 'ratsql_pred_sql' in d:\n",
    "        continue\n",
    "        \n",
    "    pred = Question(d['question'], d['db_id'])\n",
    "    if len(pred) == 0:\n",
    "        print('{}: question({}, {}) failed'.format(i, d['question'], d['db_id']))\n",
    "        d['ratsql_pred_sql'] = ''\n",
    "    else:\n",
    "        d['ratsql_pred_sql'] = pred[0]['inferred_code']\n",
    "\n",
    "asr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = question(d['question'], d['db_id'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_dataset_path, 'w') as f:\n",
    "#     json.dump(asr_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict & Evaluate SQL given rewriter output / original / ASR cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "kmaps = evaluation.build_foreign_key_map_from_json(tables_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateSQL(pred_str,\n",
    "                gold_str,\n",
    "                db,\n",
    "                db_dir='/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database/'):\n",
    "    \n",
    "    db_path = os.path.join(db_dir, db, db + \".sqlite\")\n",
    "    schema = process_sql.Schema(process_sql.get_schema(db_path))\n",
    "    try:\n",
    "        g_sql = process_sql.get_sql(schema, gold_str)  # Train #3153/18259, in 'assets_maintenance', 'ref_company_types' not found \n",
    "        p_sql = process_sql.get_sql(schema, pred_str)\n",
    "    except:\n",
    "        print('{}\\n{}\\n{}\\nprocess_sql.get_sql() failed'.format(pred_str, gold_str, db))\n",
    "        return 0, 0\n",
    "    \n",
    "    # Rebuilding... copied from official evaluate \n",
    "    kmap = kmaps[db]\n",
    "    g_valid_col_units = evaluation.build_valid_col_units(g_sql['from']['table_units'], schema)\n",
    "    g_sql = evaluation.rebuild_sql_val(g_sql)\n",
    "    g_sql = evaluation.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "    p_valid_col_units = evaluation.build_valid_col_units(p_sql['from']['table_units'], schema)\n",
    "    p_sql = evaluation.rebuild_sql_val(p_sql)\n",
    "    p_sql = evaluation.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "    \n",
    "    evaluator = evaluation.Evaluator()\n",
    "    exact_match = evaluator.eval_exact_match(p_sql, g_sql)   # will modify p_sql, g_sql\n",
    "    partials = evaluator.partial_scores\n",
    "    partial_summary_score = sum([partials[tp]['f1'] * max(partials[tp]['label_total'], partials[tp]['pred_total']) for tp in partials]) / sum([max(partials[tp]['label_total'], partials[tp]['pred_total']) for tp in partials])\n",
    "\n",
    "    return int(exact_match), partial_summary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateSQL_full(glist,\n",
    "                     plist,\n",
    "                     db_id_list,\n",
    "                     kmaps,\n",
    "                     db_dir='/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database/'):\n",
    "    # Only using 'match', not using 'exec'\n",
    "    etype = 'match'\n",
    "    \n",
    "    # with open(gold) as f:\n",
    "    #     glist = [l.strip().split('\\t') for l in f.readlines() if len(l.strip()) > 0]\n",
    "\n",
    "    # with open(predict) as f:\n",
    "    #     plist = [l.strip().split('\\t') for l in f.readlines() if len(l.strip()) > 0]\n",
    "    \n",
    "    # plist = [(\"select max(Share),min(Share) from performance where Type != 'terminal'\", \"orchestra\")]\n",
    "    # glist = [(\"SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != 'Live final'\", \"orchestra\")]\n",
    "    evaluator = evaluation.Evaluator()\n",
    "\n",
    "    levels = ['easy', 'medium', 'hard', 'extra', 'all']\n",
    "    partial_types = ['select', 'select(no AGG)', 'where', 'where(no OP)', 'group(no Having)',\n",
    "                     'group', 'order', 'and/or', 'IUEN', 'keywords']\n",
    "    entries = []\n",
    "    scores = {}\n",
    "\n",
    "    for level in levels:\n",
    "        scores[level] = {'count': 0, 'partial': {}, 'exact': 0., 'partial_summary': 0.}\n",
    "        scores[level]['exec'] = 0\n",
    "        for type_ in partial_types:\n",
    "            scores[level]['partial'][type_] = {'acc': 0., 'rec': 0., 'f1': 0.,'acc_count':0,'rec_count':0}\n",
    "\n",
    "    eval_err_num = 0\n",
    "    for p, g, db in tqdm(zip(plist, glist, db_id_list), total=len(plist)):\n",
    "        p_str = p\n",
    "        g_str = g\n",
    "        db_name = db\n",
    "        db = os.path.join(db_dir, db, db + \".sqlite\")\n",
    "        schema = process_sql.Schema(process_sql.get_schema(db))\n",
    "        g_sql = process_sql.get_sql(schema, g_str)\n",
    "        hardness = evaluator.eval_hardness(g_sql)\n",
    "        scores[hardness]['count'] += 1\n",
    "        scores['all']['count'] += 1\n",
    "\n",
    "        try:\n",
    "            p_sql = process_sql.get_sql(schema, p_str)\n",
    "        except:\n",
    "            # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "            p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\n",
    "                \"conds\": [],\n",
    "                \"table_units\": []\n",
    "            },\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [\n",
    "                False,\n",
    "                []\n",
    "            ],\n",
    "            \"union\": None,\n",
    "            \"where\": []\n",
    "            }\n",
    "            eval_err_num += 1\n",
    "            print(\"eval_err_num:{}\".format(eval_err_num))\n",
    "\n",
    "        # rebuild sql for value evaluation\n",
    "        kmap = kmaps[db_name]\n",
    "        g_valid_col_units = evaluation.build_valid_col_units(g_sql['from']['table_units'], schema)\n",
    "        g_sql = evaluation.rebuild_sql_val(g_sql)\n",
    "        g_sql = evaluation.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "        p_valid_col_units = evaluation.build_valid_col_units(p_sql['from']['table_units'], schema)\n",
    "        p_sql = evaluation.rebuild_sql_val(p_sql)\n",
    "        p_sql = evaluation.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "\n",
    "#         if etype in [\"all\", \"exec\"]:\n",
    "#             exec_score = evaluation.eval_exec_match(db, p_str, g_str, p_sql, g_sql)\n",
    "#             if exec_score:\n",
    "#                 scores[hardness]['exec'] += 1\n",
    "\n",
    "        if etype in [\"all\", \"match\"]:\n",
    "            exact_score = evaluator.eval_exact_match(p_sql, g_sql)\n",
    "            partial_scores = evaluator.partial_scores\n",
    "#             if exact_score == 0:\n",
    "#                 print(\"{} pred: {}\".format(hardness,p_str))\n",
    "#                 print(\"{} gold: {}\".format(hardness,g_str))\n",
    "#                 print(\"\")\n",
    "            scores[hardness]['exact'] += exact_score\n",
    "            scores['all']['exact'] += exact_score\n",
    "            for type_ in partial_types:\n",
    "                if partial_scores[type_]['pred_total'] > 0:\n",
    "                    scores[hardness]['partial'][type_]['acc'] += partial_scores[type_]['acc']\n",
    "                    scores[hardness]['partial'][type_]['acc_count'] += 1\n",
    "                if partial_scores[type_]['label_total'] > 0:\n",
    "                    scores[hardness]['partial'][type_]['rec'] += partial_scores[type_]['rec']\n",
    "                    scores[hardness]['partial'][type_]['rec_count'] += 1\n",
    "                scores[hardness]['partial'][type_]['f1'] += partial_scores[type_]['f1']\n",
    "                if partial_scores[type_]['pred_total'] > 0:\n",
    "                    scores['all']['partial'][type_]['acc'] += partial_scores[type_]['acc']\n",
    "                    scores['all']['partial'][type_]['acc_count'] += 1\n",
    "                if partial_scores[type_]['label_total'] > 0:\n",
    "                    scores['all']['partial'][type_]['rec'] += partial_scores[type_]['rec']\n",
    "                    scores['all']['partial'][type_]['rec_count'] += 1\n",
    "                scores['all']['partial'][type_]['f1'] += partial_scores[type_]['f1']\n",
    "\n",
    "            # Custom\n",
    "            partial_summary_score = sum([partial_scores[tp]['f1'] * max(partial_scores[tp]['label_total'], partial_scores[tp]['pred_total']) for tp in partial_types]) / sum([max(partial_scores[tp]['label_total'], partial_scores[tp]['pred_total']) for tp in partial_types])\n",
    "            scores[hardness]['partial_summary'] += partial_summary_score\n",
    "            scores['all']['partial_summary'] += partial_summary_score\n",
    "                \n",
    "            entries.append({\n",
    "                'predictSQL': p_str,\n",
    "                'goldSQL': g_str,\n",
    "                'hardness': hardness,\n",
    "                'exact': exact_score,\n",
    "                'partial': partial_scores\n",
    "            })\n",
    "\n",
    "    for level in levels:\n",
    "        if scores[level]['count'] == 0:\n",
    "            continue\n",
    "        if etype in [\"all\", \"exec\"]:\n",
    "            scores[level]['exec'] /= scores[level]['count']\n",
    "\n",
    "        if etype in [\"all\", \"match\"]:\n",
    "            scores[level]['exact'] /= scores[level]['count']\n",
    "            scores[level]['partial_summary'] /= scores[level]['count']\n",
    "            for type_ in partial_types:\n",
    "                if scores[level]['partial'][type_]['acc_count'] == 0:\n",
    "                    scores[level]['partial'][type_]['acc'] = 0\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['acc'] = scores[level]['partial'][type_]['acc'] / \\\n",
    "                                                             scores[level]['partial'][type_]['acc_count'] * 1.0\n",
    "                if scores[level]['partial'][type_]['rec_count'] == 0:\n",
    "                    scores[level]['partial'][type_]['rec'] = 0\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['rec'] = scores[level]['partial'][type_]['rec'] / \\\n",
    "                                                             scores[level]['partial'][type_]['rec_count'] * 1.0\n",
    "                if scores[level]['partial'][type_]['acc'] == 0 and scores[level]['partial'][type_]['rec'] == 0:\n",
    "                    scores[level]['partial'][type_]['f1'] = 1\n",
    "                else:\n",
    "                    scores[level]['partial'][type_]['f1'] = \\\n",
    "                        2.0 * scores[level]['partial'][type_]['acc'] * scores[level]['partial'][type_]['rec'] / (\n",
    "                        scores[level]['partial'][type_]['rec'] + scores[level]['partial'][type_]['acc'])\n",
    "\n",
    "    evaluation.print_scores(scores, etype)\n",
    "    print('================   PARTIAL MATCHING SUMMARY SCORE    ================')\n",
    "    this_scores = [scores[level]['partial_summary'] for level in levels]\n",
    "    print(\"{:20} {:<20.3f} {:<20.3f} {:<20.3f} {:<20.3f} {:<20.3f}\".format(\"partial_summary\", *this_scores))\n",
    "    print()\n",
    "    print('\\t'.join([str(scores[level]['exact']) for level in levels]))\n",
    "    print('\\t'.join([str(scores[level]['partial_summary']) for level in levels]))\n",
    "    # return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human test \n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using the 1st ASR candidate, no correction \n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Question(c['question'], _db_id, model_dict=rat_sql_asr_model_dict)[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the predictions \n",
    "# Non ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "# ASR\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/First-cands.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "#     _pred_sql = Question(c['gold_question'], _db_id, model_dict=rat_sql_asr_model_dict)[0]['inferred_code']\n",
    "    _pred_sql = Question(c['gold_question'], _db_id)[0]['inferred_code']\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_subset = random.sample(test_dataset, k=100)\n",
    "\n",
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in random_subset],\n",
    "                 glist=[d[0]['query'] for d in random_subset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in random_subset],\n",
    "                 kmaps=kmaps\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postprocess_rewrite_seq(tags, rewrite_seq, question_toks):\n",
    "    _question_toks_placeholders = []\n",
    "\n",
    "    for i, tok in enumerate(question_toks):\n",
    "        if tags[i].endswith('KEEP'):\n",
    "            _question_toks_placeholders.append(tok)\n",
    "        elif (tags[i] == 'U-EDIT') or (tags[i] == 'B-EDIT'):\n",
    "            _question_toks_placeholders.append('[EDIT]')\n",
    "        elif (tags[i] == 'I-EDIT') or (tags[i] == 'L-EDIT') or tags[i].endswith('DEL'):\n",
    "            pass\n",
    "        else:\n",
    "            print('Unknown tag: {}'.format(tags[i]))\n",
    "\n",
    "    _edits = []\n",
    "    _curr_edit = []\n",
    "    for tok in rewrite_seq:\n",
    "        if tok == '[ANS]':\n",
    "            _edits.append(_curr_edit)\n",
    "            _curr_edit = []\n",
    "        elif tok == '@end@':  # Allennlp END_SYMBOL \n",
    "            break\n",
    "        else:\n",
    "            _curr_edit.append(tok)\n",
    "    \n",
    "    _question_toks_rewritten = []\n",
    "    _edit_idx = 0\n",
    "    for tok in _question_toks_placeholders:\n",
    "        if tok == '[EDIT]':\n",
    "            if _edit_idx >= len(_edits):\n",
    "                print('--- Not enough edits ---')\n",
    "                print('Tags:', tags)\n",
    "                print('Edits:', _edits)\n",
    "            else:\n",
    "                _question_toks_rewritten.extend(_edits[_edit_idx])\n",
    "            _edit_idx += 1\n",
    "        else:\n",
    "            _question_toks_rewritten.append(tok)\n",
    "\n",
    "    return _question_toks_rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "\n",
    "rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human test \n",
    "VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "\n",
    "rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "    rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _p in rewriter_ILM_preds[::100]:\n",
    "    print(_p['rewrite_seq_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    p = rewriter_ILM_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "    # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "    \n",
    "    _tags = p['rewriter_tags']\n",
    "    _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    _question_toks = c['question_toks']\n",
    "\n",
    "    _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "    # _pred_sql = Question(_rewritten_question, _db_id, model_dict=rat_sql_asr_model_dict)[0]['inferred_code']\n",
    "    _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    # Save the taggerILM raw outputs, for later aggregation \n",
    "    c['pred_tags'] = p['rewriter_tags']\n",
    "    c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "    c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "    \n",
    "    # Save prediction results \n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "\n",
    "    _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_rewritten_question_toks)\n",
    "    \n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non ASR\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from predicted file (only 1st cand is predicted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.6.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using EvaluateSQL_full \n",
    "\n",
    "# tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "# kmaps = evaluation.build_foreign_key_map_from_json(tables_json)\n",
    "\n",
    "plist = [d[0]['pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reranker output file \n",
    "VERSION = '1.10.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [c['ratsql_pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection'] == 1]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [d[0]['ratsql_pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using all ASR candidates (no longer in use)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']\n",
    "        _tags = p['tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation process with oracle tags (no longer in use for version>=2.3.0)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _tags = p['gold_tags']\n",
    "        _rewrite_seq = p['oracle_tags_rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question'] = _rewritten_question\n",
    "        c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql'] = _pred_sql\n",
    "        c['oracle_tags_score'] = p['oracle_tags_score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['oracle_tags_score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['oracle_tags_score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_oracle_avg_1st = sum([d[0]['oracle_tags_score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_oracle_std_1st = np.std([d[0]['oracle_tags_score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_oracle_avg_1st, _oracle_std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['tag_prediction'] = list(zip(p['question'].split(' '), p['tags_prediction']))\n",
    "        _pred_c['rewrite_seq'] = []\n",
    "        for t in p['rewrite_seq_prediction']:\n",
    "            _pred_c['rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['rewritten_question'] = p['rewritten_question']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_tags'] = list(zip(p['question'].split(' '), p['gold_tags']))\n",
    "        _pred_c['oracle_tags_rewrite_seq'] = []\n",
    "        for t in p['oracle_tags_rewrite_seq_prediction']:\n",
    "            _pred_c['oracle_tags_rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question']\n",
    "        _pred_c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql']\n",
    "        _pred_c['oracle_tags_score'] = p['oracle_tags_score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test-prediction-{}.json'.format(VERSION), 'w') as f:\n",
    "    json.dump(test_pred_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset file with predictions \n",
    "\n",
    "with open('./output/pred-{}.json'.format(VERSION), 'r') as f:\n",
    "    test_pred_dataset = json.load(f)\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis \n",
    "orig_dev_preds_path = './output/dev_output.txt'\n",
    "\n",
    "with open(orig_dev_preds_path, 'r') as f:\n",
    "    orig_dev_preds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "len(orig_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for d in test_dataset[5::40]:\n",
    "#     print('DB:', d[0]['db_id'])\n",
    "#     print('ASR question:\\t\\t', d[0]['question'])\n",
    "#     print('Rewritten question:\\t', d[0]['rewritten_question'])\n",
    "#     print('Gold question:\\t\\t', d[0]['gold_question'])\n",
    "#     print('ASR-q Pred SQL:\\t\\t', d[0]['ratsql_pred_sql'])\n",
    "#     print('Rewritten-q Pred SQL:\\t', d[0]['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', orig_dev_preds[d[0]['original_id']])\n",
    "#     print('Gold SQL:\\t\\t', d[0]['query'])\n",
    "#     print('Score:', d[0]['score'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_samples = [c for d in test_pred_dataset for c in d]\n",
    "for i, c in list(enumerate(test_pred_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['ASR_question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "    print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['gold_sql'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = [8, 96, 272, 448, 1416, 1592, 1680, 1856, 2120, 2296, 2384, 2560, 2824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Batch evaluating \n",
    "\n",
    "VERSION_LIST = ['3.1.1_ep0', '3.1.1.1_ep0', '3.1.1.2_ep0']\n",
    "HUMAN_TEST = True\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    print(f'Evaluating{\" (human test)\" if HUMAN_TEST else \"\"}: VERSION = {VERSION}')\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        # human test \n",
    "        rewriter_s2s_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    \n",
    "    with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "        rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "        \n",
    "    # Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    \n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags']\n",
    "        # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        # _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "\n",
    "        if _rewritten_question == '':\n",
    "            print(f'_rewritten_question is empty')\n",
    "            _pred_sql = ''\n",
    "            _gold_sql = c['query']\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "            _gold_sql = c['query']\n",
    "            _exact, _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        \n",
    "        # For BLEU \n",
    "        _rewritten_question_toks = [_t.lower() for _t in p['s2s_prediction']]\n",
    "        _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "    \n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    \n",
    "    print(f'VERSION {VERSION}:')\n",
    "    print(f'avg = {_avg_1st:.4f} (std = {_std_1st:.4f})')\n",
    "    print(f'avg_exact = {_avg_exact_1st:.4f}')\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print()\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "        with open(test_output_path, 'w') as f:\n",
    "            json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.3.0.0'\n",
    "\n",
    "rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "    rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_preds[3::300]:\n",
    "    print(p['question'])\n",
    "    print(' '.join(p['s2s_prediction']))\n",
    "    print(' '.join(p['gold_rewrite_seq_s2s'][1:-1]))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags']\n",
    "    # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    # _question_toks = c['question_toks']\n",
    "\n",
    "    # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "    \n",
    "    if _rewritten_question == '':\n",
    "        print(f'_rewritten_question is empty')\n",
    "        _pred_sql = ''\n",
    "        _gold_sql = c['query']\n",
    "        _score = 0\n",
    "    else:\n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "\n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actual (full) evaluation process \n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        pred_idx += 1\n",
    "        if 'score' in c:\n",
    "            continue  # already inferred  \n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "        _pred_result = Question(_rewritten_question, _db_id)\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        \n",
    "        if len(_pred_result) == 0:\n",
    "            print(_db_id, _rewritten_question, '-- no predictiction')\n",
    "            _pred_sql = ''\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = _pred_result[0]['inferred_code']\n",
    "            _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_preds]) / len(rewriter_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['rewritten_question'] = p['s2s_prediction']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [c for d in test_dataset for c in d]\n",
    "for i, c in list(enumerate(test_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['query'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_ILM_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dev_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite_seq postprocessing, to get the rewritten question \n",
    "\n",
    "_idx = 154\n",
    "\n",
    "_tags = rewriter_ILM_preds[_idx]['tags_prediction']\n",
    "_rewrite_seq = rewriter_ILM_preds[_idx]['rewrite_seq_prediction']\n",
    "_question_toks = rewriter_ILM_preds[_idx]['question'].split(' ')\n",
    "_tags, _rewrite_seq, _question_toks\n",
    "\n",
    "postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'concert_singer'\n",
    "g_str = 'SELECT count(*) FROM singer'\n",
    "p_str = \"SELECT Count(DISTINCT singer.Name) FROM singer WHERE singer.Name = 'terminal'\"\n",
    "\n",
    "db, p_str, g_str, EvaluateSQL(p_str, g_str, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
